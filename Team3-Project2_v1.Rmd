---
title: "Analysis of Robbery Rates in Washington D.C: Identifying Factors that Influence Police Efficiency and Severity of Robbery"
author: "Muhannad Alwhebie, Liang Gao, Nammin Woo"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=F}
library(ezids)
library(readr) 
library(ggplot2)
library(DBI) 
library(magrittr)
library(dplyr)  
library(tidyr) 
library(tidyverse)
library(graphics) 
library(gridExtra) 
library(data.table)
library(ROCR)
library(grid)
library(broom)
library(scales)
library(ggthemes)
library(psych)
loadPkg("pROC") 
loadPkg("rpart")
library(corrplot)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
options(scipen=9999, digits = 3) 
```

## Abstract


## Introduction 

This project aims to build upon the previous exploratory data analysis project that examined the impact of COVID-19 lockdown on robbery rates in Washington D.C. In this project, our objective is to identify factors that influence police efficiency and severity of robbery crime. To gain a more comprehensive understanding of the underlying factors affecting robbery rates, we plan to incorporate demographic and economic data from the Census Bureau, such as median household income, poverty rate, and education level, as predictor variables in our model.

We will use two target variables in our analysis: Target_Clearance_time (whether the robbery took longer to clear) and Target_robbery_method (whether the robbery involved a gun or another type of weapon). By analyzing the relationships between these target variables and other explanatory variables, we hope to gain insights into the factors that affect clearance time and the use of guns in robberies.

The previous project found changes in robbery frequency during the pandemic, with underlying socio-economic factors potentially contributing to the patterns. Additionally, it supported the findings of the Boman and Gallupe (2020) study that violent crime increased during the lockdown period, with a dramatic increase in the number of robberies during Autumn. By utilizing GLM models to answer SMART questions related to robbery rates, we aim to gain a deeper understanding of the underlying factors that affect robbery rates in Washington D.C and provide insights that can inform future policy decisions related to crime prevention and public safety.

**Goal of this project: Building models to identify factors that influence police efficiency and severity of robbery.**



**Our SMART questions for this project include:**

1. What are the most important features that influence the clearance time and severity of robbery methods?

2. How accurately can we predict the clearance time and method based on these features?


## Methodology

*Data Sourcing*:

The dataset used in this study was sourced from the Open Data DC repository, which contains a collection of publicly available datasets related to various aspects of life in Washington D.C. Specifically, we used a dataset containing roughly 8000 observations of robbery incidents in the District of Columbia between January 2019 and December 2022. In addition, we obtained the Economic Characteristics of DC Census Tracts surveyed by the U.S. government using the CENSUS_TRACT variable.

*Data Preprocessing:*

After obtaining the necessary data, we first aggregated the census bureau data to the original dataset to include economic characteristics. Next, we defined two target variables, Target_Clearance_time and Target_robbery_method, which represent whether the robbery took longer to clear and whether a gun was used or not, respectively. We then conducted exploratory data analysis (EDA) to gain insights into the data and performed pre-processing to prepare the data for predictive modeling.

*Feature Selection:*
We used correlation analysis to identify the features that have the strongest relationship with the target variables. We also performed EDA on the target variables to understand their distribution and behavior with respect to the candidate features.

*Model Building and Evaluation:*
We used a split/train method to build a predictive model using the selected features. We evaluated the model's performance using various metrics such as accuracy, precision, recall, and F1 score. We also interpreted the model to gain insights into the most important features that influence the target variables.


Overall, the methodology used in this study involved data sourcing, preprocessing, feature selection, model building, and evaluation. The inclusion of economic characteristics data and the use of machine learning techniques can help to provide insights into the underlying factors that influence robbery rates in Washington D.C.


### 1. Data sourcing

Load the latest data in project 1. The outliers of clearance_rate have been removed (converted into NA) in Robbery_clean.

```{r}
Robbery <- 
  readr::read_rds("Robbery_230303.Rds")

Robbery_clean <- 
  readr::read_rds("Robbery_clean_230303.Rds")
```

To help into predictive modeling, we decided to add a few new features that come from census bureau. 
```{r}
#Import
Robbery_clean_p2 = data.frame(read.csv("Robbery_clean_p2.csv"))
Robbery_p2 = data.frame(read.csv("Robbery_p2.csv"))
Census_eco = data.frame(read.csv("ACS_Economic_Census_clean.csv"))

```

This is original dataset we will use.

```{r}
head(Robbery_clean_p2,5) 
```


This is census data we will add on to original dataset.
All robbery records in our dataset have its own location information that the robberies are occured.

We used location variable `DISTRICT` in project 1 meaning Police Service Area, and decided the additional location variable `CENSUS_TRACT` meaning statistical subdivisions of a 
county.(Population from 1,200 to 1,800)

And using `CENSUS_TRACT`, we additionally got the **specific Economic Characteristics of DC Census Tracts** that are surveyed from U.S government. 

Economic Characteristics are about Employment, Commuting, Occupation, Income, Health Insurance, Poverty,etc., and it is basaed on the most current release of data from the American Community Survey (ACS) about economic characteristics. (2020 Census Tract boundaries)

```{r}
head(Robbery_clean_p2,5) 
#str(Census_eco)
```

```{r}
#columns that are necessary to anaysis
#ex_cols <- c('OBJECTID','STATEFP','COUNTYFP','CENSUS_TRACT','GEOID')
temp <- select(Census_eco,-OBJECTID,-STATEFP,-COUNTYFP,-CENSUS_TRACT,-GEOID)

knitr::kable(temp, format = "markdown", caption="Overall summary statistics of DC Census Tract(Economic Characteristics)")

```

Aggregate census bureau data to the original dataset.

```{r}
#Add new features joining with key
temp <- select(Census_eco,-OBJECTID,-STATEFP,-COUNTYFP,-GEOID)
Robbery_clean_p2_add <- merge(x = Robbery_clean_p2, y = temp, by = "CENSUS_TRACT", all.x = TRUE)
```

```{r}
count(Robbery_clean_p2,SHIFT)
```


```{r}
head(temp,5) 
```


Verify the aggregation (number of rows and columns)

```{r}
nrow(Robbery_clean_p2)
nrow(Robbery_clean_p2_add)
```


```{r}
ncol(Robbery_clean_p2) #Before aggregation
ncol(Census_eco)-5     #Census 
ncol(Robbery_clean_p2_add) #After aggregation(add Cesnsus)
```

```{r}
head(Robbery_clean_p2_add,5) 
```

### 2. Define target variables

To define the target variables for our predictive modeling, we first need to understand what we are trying to predict. In this case, we are interested in predicting the clearance time and the method of robbery.

The first target variable we will define is `Target_Clearance_time.` This variable will indicate whether a robbery took longer to clear or not. The clearance time will be defined as the time between the initial report of the robbery and the time when the case is closed. If the clearance time is longer than the median clearance time for all cases in the dataset, the value for `Target_Clearance_time` will be 1, indicating that the robbery took longer to clear. If the clearance time is shorter than the median, the value for `Target_Clearance_time` will be 0, indicating that the robbery did not take longer to clear.

The second target variable we will define is `Target_robbery_method.` This variable will indicate whether the robbery was committed with a gun or with some other weapon or no weapon at all. We will use the "Gun" variable from the dataset to determine the robbery method. If the weapon used was a gun, the value for Target_robbery_method will be 1, indicating that the robbery was committed with a gun. If the weapon used was anything else, the value for `Target_robbery_method` will be 0, indicating that the robbery was committed with some other weapon or no weapon at all.

By defining these two target variables, we can build predictive models that will allow us to identify the factors that contribute to longer clearance times and gun-related robberies. We can then use this information to develop strategies for reducing these types of crimes in Washington D.C.


```{r}
#Summary
#columns to analyse 
cols <- c('Clearance_rate','METHOD')
#summary(select(Robbery,cols))
summary(select(Robbery_clean_p2_add,cols))        

```

1. `Target_Clearance_time`: whether need Longer time to clear or not


```{r}
plot <- ggplot(data=Robbery_clean_p2_add, aes(Clearance_rate)) + 
  geom_histogram(col="darkgrey", 
                 fill="lightgrey", 
                 alpha = .7,bins = 30) + 
  labs(x="Clearance_rate", y="Frequency") +
  labs(title="Clearance_rate frequency") 
plot
```

We defined Long clearance time as over 85 min (75% quantile, `Target_Clearance_time` = 1).
But will check detail in the stage of modeling.

```{r}
Robbery_clean_p2_add$Target_Clearance_time <- ifelse(Robbery_clean_p2_add$Clearance_rate >= 85,
c(1), c(0))

```



At this stage, we have three levels for for `Target_Clearance_time`. 
  + `Target_Clearance_time`=1 : Need more than 85 min (75% quantile) 
  + `Target_Clearance_time`=0 : Need less than 85 min 
  +  NA: `Outliers` (Need extremely long time (avg 2.5 days), will not use in modeling



```{r}
count(Robbery_clean_p2_add,Target_Clearance_time, sort=FALSE)
```

2. `Target_robbery_method`: Gun or other

```{r}
# 1
Robbery_clean_p2_add$Target_robbery_method <- ifelse(Robbery_clean_p2_add$METHOD == 'GUN',c(1), c(0))

```

```{r}
count(Robbery_clean_p2_add,Target_robbery_method, sort=FALSE)
```

Temporary Saved current data.

```{r}
#Dataframe Save and load
#readr::write_rds(Robbery_clean_p2_add, "Robbery_clean_p2_add.Rds")

Robbery_clean_p2_add <- 
  readr::read_rds("Robbery_clean_p2_add.Rds")
```


### 3. Pre-processing for predictive modeling

From this section, we will check only input/target variables that are used for modeling.

Check data type of original variables (With or Without Census Track)

A. To use `Period_cat` in our model, convert into two categories (Lock-down=1, other=0).

```{r}
attach(Robbery_clean_p2_add)
Robbery_clean_p2_add$Period_cat_num_2[Period_cat == 'Pre'] <- 0
Robbery_clean_p2_add$Period_cat_num_2[Period_cat == 'Lock'] <- 1
Robbery_clean_p2_add$Period_cat_num_2[Period_cat  == 'Post'] <- 0
detach(Robbery_clean_p2_add)
summary(Robbery_clean_p2_add$Period_cat_num_2)
```


```{r}
col_org <- c('Date_cat','Day_cat','Day_cat_2','DISTRICT','Month','Period_cat_num_2','Season','SHIFT','METHOD','Clearance_rate','Target_robbery_method','Target_Clearance_time')

```

Check data type of original variables (w.o Census)

```{r}
str(select(Robbery_clean_p2_add,col_org)) 

#sum_list <- summary(Robbery_clean_p2_add)   
#write.csv(sum_list, "sum_list.csv", row.names=FALSE)
```

B. Changed categorical variables (including target) into factor. 

```{r}
cols <- c('Date_cat','Day_cat_2','Season','SHIFT','METHOD','Period_cat_num_2','Target_Clearance_time','Target_robbery_method')  
#array
Robbery_clean_p2_add %<>%
       mutate_each_(funs(factor(.)),cols) 
```

Verified conversion of data type.

```{r}
str(select(Robbery_clean_p2_add,col_org)) 
```



C. Changed Census variables into numeric. 


```{r}
#15 vars 
cols <- c('females_employed_rate','hs_10000_to_14999_pct','hs_100000_to_149999_pct','hs_15000_to_24999_pct','hs_150000_to_194999_pct','hs_200000__pct','hs_25000_to_34999_pct','hs_35000_to_49999_pct','hs_50000_to_74999_pct','hs_75000_to_99999_pct','hs_Less_than_10000_pct','hs_own_children_6_to_17_labor_force_pct','hs_own_children_6_to_17_pct','hs_own_children_under_6_labor_force_pct','hs_own_children_under_6_pct')  #array
Robbery_clean_p2_add[cols] <- sapply(Robbery_clean_p2_add[cols],as.numeric)
```


Verified conversion of data type.
`r nrow(Robbery_clean_p2_add)`records in data.


```{r}
str(select(Robbery_clean_p2_add,cols)) 
#sum_list <- summary(select(Robbery_clean_p2_add,cols))   
#write.csv(sum_list, "sum_list_rv.csv", row.names=FALSE)
```

D. Additional input var (`Hour`): Start time of police clearance.

```{r}
#colnames(Robbery_clean_p2_model)
library(lubridate)
head(Robbery_clean_p2_add$START_DATE,5)
Robbery_clean_p2_add$Hour <- hour(Robbery_clean_p2_add$START_DATE)
```

```{r}
co = c('START_DATE','Hour')
head(select(Robbery_clean_p2_add,co),5)
```


Temporary Saved current data.

```{r}
#Dataframe Save and load
#readr::write_rds(Robbery_clean_p2_add, "Robbery_clean_p2_model.Rds")

Robbery_clean_p2_model <- 
  readr::read_rds("Robbery_clean_p2_model.Rds")
```



**E.Final modeling data 2.(`Target_robbery_method`)**

To finalize modeling data 2 for `Target_robbery_method`, we do not need to exclude records(there are no outliers). So just maintain the initial data. (8343 records)

```{r}
count(Robbery_clean_p2_model,Target_robbery_method, sort=FALSE)
```


And finalize modeling data 2 , 1 target and 86 input candidate variables (including 78 variables are Census).

```{r}

  cols_m2 <- c('Target_robbery_method','Hour','Date_cat','Day_cat','Day_cat_2','DISTRICT','Month','Period_cat_num_2','Season','SHIFT','Agriculture_forestry_fishing_and_hunting_and_mining_pct','armed_forces_rate','Arts_entertainment_and_recreation_and_accommodation_and_food_services_pct','carpooled_pct','Construction_pct','drove_alone_pct','Earning_mean','Earning_median_fmale_wk','Earning_median_male_wk','Earning_median_wk','Educational_services_and_health_care_and_social_assistance_pct','females_employed_rate','Finance_and_insurance_and_real_estate_and_rental_and_leasing_pct','Government_workers_pct','hs_10000_to_14999_pct','hs_100000_to_149999_pct','hs_15000_to_24999_pct','hs_150000_to_194999_pct','hs_200000__pct','hs_25000_to_34999_pct','hs_35000_to_49999_pct','hs_50000_to_74999_pct','hs_75000_to_99999_pct','hs_Less_than_10000_pct','hs_own_children_6_to_17_labor_force_pct','hs_own_children_6_to_17_pct','hs_own_children_under_6_labor_force_pct','hs_own_children_under_6_pct','Income_mean','Income_median','Information_pct','labor_forces_rate','management_business_science_arts_occupations_pct','Manufacturing_pct','mean_travel_time','Natural_resources_construction_and_maintenance_occupations_pct','other_means_pct','Other_services_except_public_administration_pct','pct_all_families_below_poverty','pct_all_families_related_children_below18_below_poverty','pct_all_families_related_children_below5_below_poverty','pct_all_people_18plus_below_poverty','pct_all_people_18to64_below_poverty','pct_all_people_65plus_below_poverty','pct_all_people_below_poverty','pct_all_people_related_children_under18_5to17_below_poverty','pct_all_people_related_children_under18_below_poverty','pct_all_people_related_children_under18_below5_below_poverty','pct_all_people_under18_below_poverty','pct_below_poverty_female_no_spouse_present','pct_below_poverty_female_no_spouse_present_related_children_below18','pct_female_no_spouse_present_related_children_below18_below5_below_poverty','pct_married_couple_families_below_poverty','pct_married_couple_families_related_children_below18_below_poverty','pct_married_couple_families_related_children_below5_below_poverty','pct_people_in_families_below_poverty','pct_unrelated_individuals_15plus_below_poverty','pop_16_over_emp_status','Private_wage_and_salary_workers_pct','Production_transportation_and_material_moving_occupations_pct','Professional_scientific_and_management_and_administrative_and_waste_management_services_pct','Public_administration_pct','public_assistance_income_mean','public_transportation_pct','Retail_trade_pct','Retirement_income_mean','Sales_and_office_occupations_pct','Self_employed_in_own_not_incorporated_business_workers_pct','service_occupations_pct','Social_Security_income_mean','Supplemental_Security_income_mean','Transportation_and_warehousing_and_utilities_pct','unemployment_rate','Unpaid_family_workers_pct','walked_pct','Wholesale_trade_pct','worked_from_home_pct','workers_16_over')

model_2_clean <- select(Robbery_clean_p2_model,cols_m2)
#str(model_2_clean)
```


```{r}
#Dataframe Save and load
#readr::write_rds(model_2_clean, "Robbery_clean_p2_model_2.Rds")

#model_2_clean <- readr::read_rds("Robbery_clean_p2_model_2.Rds")
```

`r nrow(model_2_clean)`records,`r ncol(model_2_clean)` variables in data 2.
Include 1 target and 78 Census variables.
Save finalized dataset for model 1. Now, it's ready to Modeling.



**F.Final modeling data 1.(`Target_Clearance_time`)**


```{r}
cols_m1 <- c('Clearance_rate','Target_Clearance_time','Hour','Date_cat','Day_cat','Day_cat_2','DISTRICT','Month','Period_cat_num_2','Season','SHIFT','METHOD','Agriculture_forestry_fishing_and_hunting_and_mining_pct','armed_forces_rate','Arts_entertainment_and_recreation_and_accommodation_and_food_services_pct','carpooled_pct','Construction_pct','drove_alone_pct','Earning_mean','Earning_median_fmale_wk','Earning_median_male_wk','Earning_median_wk','Educational_services_and_health_care_and_social_assistance_pct','females_employed_rate','Finance_and_insurance_and_real_estate_and_rental_and_leasing_pct','Government_workers_pct','hs_10000_to_14999_pct','hs_100000_to_149999_pct','hs_15000_to_24999_pct','hs_150000_to_194999_pct','hs_200000__pct','hs_25000_to_34999_pct','hs_35000_to_49999_pct','hs_50000_to_74999_pct','hs_75000_to_99999_pct','hs_Less_than_10000_pct','hs_own_children_6_to_17_labor_force_pct','hs_own_children_6_to_17_pct','hs_own_children_under_6_labor_force_pct','hs_own_children_under_6_pct','Income_mean','Income_median','Information_pct','labor_forces_rate','management_business_science_arts_occupations_pct','Manufacturing_pct','mean_travel_time','Natural_resources_construction_and_maintenance_occupations_pct','other_means_pct','Other_services_except_public_administration_pct','pct_all_families_below_poverty','pct_all_families_related_children_below18_below_poverty','pct_all_families_related_children_below5_below_poverty','pct_all_people_18plus_below_poverty','pct_all_people_18to64_below_poverty','pct_all_people_65plus_below_poverty','pct_all_people_below_poverty','pct_all_people_related_children_under18_5to17_below_poverty','pct_all_people_related_children_under18_below_poverty','pct_all_people_related_children_under18_below5_below_poverty','pct_all_people_under18_below_poverty','pct_below_poverty_female_no_spouse_present','pct_below_poverty_female_no_spouse_present_related_children_below18','pct_female_no_spouse_present_related_children_below18_below5_below_poverty','pct_married_couple_families_below_poverty','pct_married_couple_families_related_children_below18_below_poverty','pct_married_couple_families_related_children_below5_below_poverty','pct_people_in_families_below_poverty','pct_unrelated_individuals_15plus_below_poverty','pop_16_over_emp_status','Private_wage_and_salary_workers_pct','Production_transportation_and_material_moving_occupations_pct','Professional_scientific_and_management_and_administrative_and_waste_management_services_pct','Public_administration_pct','public_assistance_income_mean','public_transportation_pct','Retail_trade_pct','Retirement_income_mean','Sales_and_office_occupations_pct','Self_employed_in_own_not_incorporated_business_workers_pct','service_occupations_pct','Social_Security_income_mean','Supplemental_Security_income_mean','Transportation_and_warehousing_and_utilities_pct','unemployment_rate','Unpaid_family_workers_pct','walked_pct','Wholesale_trade_pct','worked_from_home_pct','workers_16_over')

model_1_clean <- select(Robbery_clean_p2_model,cols_m1)
#str(model_1_clean)
#nrow(model_1_clean)
#ncol(model_1_clean)
```



Exclude **Outliers of `Target_Clearance_time`** having value Na. (We did Unusual Observations test using `outlierKD2` in project 1, and outliers of Clearance_rate that have extremely large value were converted into Na).



```{r}
model_1_clean <-  subset(model_1_clean, Target_Clearance_time == 0 |Target_Clearance_time == 1)

nrow(model_1_clean)
#NA: extremely long time is long time (or remove all NAs from data)
#Robbery_clean_p2_add$Target_Clearance_time(is.na(Robbery_clean_p2_add$Target_Clearanc#e_time))=1
count(model_1_clean,Target_Clearance_time, sort=FALSE)
#str(model_1_clean)
```




**Let Check detail about `Target_Clearance_time` by examining relationship with independent vars.**

Distribution of `Clearance_rate` after cleansing outliers.


```{r}
plot <- ggplot(data=model_1_clean, aes(Clearance_rate)) + 
  geom_histogram(col="darkgrey", 
                 fill="lightgrey", 
                 alpha = .7,bins = 30) + 
  labs(x="Clearance_rate", y="Frequency") +
  labs(title="Clearance_rate frequency") 
plot
```


```{r}
summary(model_1_clean$Clearance_rate)
```


First, display `avg.Clearance_time` per categorical variables that have many levels.

Average of clearance_time v.s DISTRICT

```{r}
#TWO var Groupby(mean of target by X category groups)
temp <-  model_1_clean %>% group_by(DISTRICT) %>%
                   summarise(avg = mean(Clearance_rate, na.rm = TRUE),.groups = 'drop')  #Targe(%): mean( as.numeric(Target_Clearance_time)

arrange(temp, DISTRICT) 
temp_gr <- data.frame(temp)

#Make chart (scatter)
plot <- ggplot(temp_gr, aes(DISTRICT, avg)) + geom_line() + labs(x="district", y="Avg.Clearance time(min)") +
  labs(title="Avg.Clearance time by district") + scale_x_discrete(limits=c("1","2","3","4","5","6","7")) + geom_point() #with 3-D: ggplot(temp_gr, aes(x, y, colour = Period_cat))  

plot
```



Hour.

```{r}
temp <-  model_1_clean %>% group_by(Hour) %>%
                   summarise(avg = mean(Clearance_rate, na.rm = TRUE),.groups = 'drop')  #Targe(%): mean( as.numeric(Target_Clearance_time)

arrange(temp, Hour) 
temp_gr <- data.frame(temp)

#Make chart (scatter)
plot <- ggplot(temp_gr, aes(Hour, avg)) + geom_line() + labs(x="Hour", y="Avg.Clearance time(min)") +
  labs(title="Avg.Clearance time by Hour") + geom_point() 

plot
```



Month.

```{r}
temp <-  model_1_clean %>% group_by(Month) %>%
                   summarise(avg = mean(Clearance_rate, na.rm = TRUE),.groups = 'drop')  #Targe(%): mean( as.numeric(Target_Clearance_time)

arrange(temp, Month) 
temp_gr <- data.frame(temp)

#Make chart (scatter)
plot <- ggplot(temp_gr, aes(Month, avg)) + geom_line() + labs(x="Month", y="Avg.Clearance time(min)") +
  labs(title="Avg.Clearance time by Month") + geom_point() 

plot
```

Next, Following is Boxplots of `Clearance_rate` for the rest categorical variables. 

```{r}
box <- ggplot(model_1_clean, aes(y=Clearance_rate, x=SHIFT)) + 
  geom_boxplot() + 
  geom_boxplot( colour="darkblue", outlier.colour="lightblue", outlier.shape=8, outlier.size=1) +
  labs(title="Clearance_rate by SHIFT",x="SHIFT", y = "Clearance_rate")
box
```

```{r}
box <- ggplot(model_1_clean, aes(y=Clearance_rate, x=Season)) + 
  geom_boxplot() + 
  geom_boxplot( colour="darkblue", outlier.colour="lightblue", outlier.shape=8, outlier.size=1) +
  labs(title="Clearance_rate by Season",x="Season", y = "Clearance_rate")
box
```


```{r}
#Dataframe Save and load
#readr::write_rds(model_2_clean, "Robbery_clean_p2_model_2.Rds")

model_1_clean <- readr::read_rds("Robbery_clean_p2_model_1.Rds")
```

```{r}
box <- ggplot(model_1_clean, aes(y=Clearance_rate, x=Period_cat_num_2)) + 
  geom_boxplot() + 
  geom_boxplot( colour="darkblue", outlier.colour="lightblue", outlier.shape=8, outlier.size=1) +
  labs(title="Clearance_rate by Lockdown",x="Period_cat_num_2", y = "Clearance_rate")
box
```

```{r}
box <- ggplot(model_1_clean, aes(y=Clearance_rate, x=Date_cat)) + 
  geom_boxplot() + 
  geom_boxplot( colour="darkblue", outlier.colour="lightblue", outlier.shape=8, outlier.size=1) +
  labs(title="Clearance_rate by Date_cat",x="Date_cat", y = "Clearance_rate")
box
```

```{r}
box <- ggplot(model_1_clean, aes(y=Clearance_rate, x=Day_cat_2)) + 
  geom_boxplot() + 
  geom_boxplot( colour="darkblue", outlier.colour="lightblue", outlier.shape=8, outlier.size=1) +
  labs(title="Clearance_rate by Day_cat_2",x="Day_cat_2", y = "Clearance_rate")
box
```

Method.

```{r}
box <- ggplot(model_1_clean, aes(y=Clearance_rate, x=METHOD)) + 
  geom_boxplot() + 
  geom_boxplot( colour="darkblue", outlier.colour="lightblue", outlier.shape=8, outlier.size=1) +
  labs(title="Clearance_rate by METHOD",x="METHOD", y = "Clearance_rate")
box
```
METHOD splitted by Period_cat_num_2(lockdown or not). 
Clearance time by Method shows very large fluctuation by lock-down. so it does not show significant relationship with target Clearance time in etire period.

```{r}
#TWO var Groupby(mean of target by X category groups)= % of High Clearance_time
temp <-  model_1_clean %>% group_by(METHOD,Period_cat_num_2) %>%
                   summarise(avg = mean(Clearance_rate, na.rm = TRUE),.groups = 'drop')

arrange(temp, METHOD,Period_cat_num_2) 

temp_gr <- data.frame(temp)

#Make chart (scatter)
plot_2 <- ggplot(temp_gr, aes(METHOD, avg, colour = Period_cat_num_2)) + geom_line() + labs(x="METHOD", y="Avg.Clearance time(min)") +
  labs(title="Avg.Clearance time(min) by METHOD and Period") + geom_point()  

plot_2

```


**Every charts indicate we had better change threshold of clearance_rate.**

Clearance_rate is highly right skewed, and main variables are showing difference around 40~50 line. So, decided to change definition of `Target_Clearance_time` from 85 min(Q3) to 40min.
But, will be keep the orginal target by the stage of modeling.

```{r}
# 1
model_1_clean$Target_Clearance_time_f <- ifelse(model_1_clean$Clearance_rate >= 40,
c(1), c(0))

#2. This one is optional (GUN&Clear)
model_1_clean$Target_clr_gun <- ifelse((model_1_clean$Target_Clearance_time_f ==1) & (model_1_clean$METHOD =='GUN'), c(1),c(0))

```

Check before moving on to the next.

```{r}
count(model_1_clean,Target_Clearance_time_f, sort=FALSE)
```

```{r}
#convert into factor
cols <- c('Target_Clearance_time_f', 'Target_clr_gun')  
#array
model_1_clean %<>%
       mutate_each_(funs(factor(.)),cols) 
#str(model_1_clean)
```


Temporary Saved current data.

```{r}
#Dataframe Save and load
#readr::write_rds(model_1_clean, "Robbery_clean_p2_model_1.Rds")

model_1_clean <- 
  readr::read_rds("Robbery_clean_p2_model_1.Rds")
```

`r nrow(model_1_clean)`records,`r ncol(model_1_clean)` variables in data 1.
Include 1 target and 78 Census variables.
Save finalized dataset for model 1. Now, it's ready to Modeling.


### 4. Model 1- Logistic regression(`Target_Clearance_time`)

 + Data: model_1_clean
 + Target: `Target_Clearance_time_f` (over 40 min)


**1. Pick the input variables for modeling**

 + Correlation between candidate variables and target variable.
 + Related EDA: Scatter charts w target, Confusion matrix etc.


**A. Correlation- original variables (w.o Census)**

Most origin variables will be used in the model. 

District(Police Service Area), Period_cat_num_2(If Pandemic period) and SHIFT(Timezone of robberies) show correlation with Clearance_target, but it is small.
Day_Cat does not have relationship so we will not use that.


```{r}

pairs.panels(model_1_clean[,c(91,3:12)],   
             method = "pearson",
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )

```
Try to find statistical clue by Chi-square test.
Test result shows significance.

DISTRICT.

```{r}
contable = table(model_1_clean$DISTRICT, model_1_clean$Target_Clearance_time_f)
chitest = chisq.test(contable)
chitest

```

In Chi-squared test of independence betwwen DISTRICT and Target_Clearance_time_f,

  + $\chi^2$ value of the test is **`r chitest$statistic`**
  + The p-value is **0.1**, which is > $\alpha$ (0.05), set to the significant level.
  + **So, The data present it does not have enough evidence to reject H0.** 

SHIFT.

```{r}
contable = table(model_1_clean$SHIFT, model_1_clean$Target_Clearance_time_f)
chitest = chisq.test(contable)
chitest

```
Period_cat_num_2

```{r}
contable = table(model_1_clean$Period_cat_num_2, model_1_clean$Target_Clearance_time_f)
chitest = chisq.test(contable)
chitest

```
A few variables like `Hour` is not significant (corr = 0.02). But we will use it for prediction.

```{r}
contable = table(model_1_clean$Hour, model_1_clean$Target_Clearance_time_f)
chitest = chisq.test(contable)
chitest

```

**B. Correlation- Census variables **

1. By examining correlation plot matrix for Census variables, initally picked up variables that have correlation with target over 0.1

(#1~8) drove_alone_pct,Earning_mean,Earning_median_fmale_wk

```{r}
pairs.panels(model_1_clean[,c(91,13:20)],   
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```

(#9~18) Earning_median_male_wk,Earning_median_wk,females_employed_rate
,hs_10000_to_14999_pct,hs_100000_to_149999_pct,hs_15000_to_24999_pct, hs_150000_to_194999_pct


```{r}
pairs.panels(model_1_clean[,c(91,21:30)],  
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE, 
             ellipses = FALSE 
             )
```
```{r}
str(model_1_clean)
```


(#19~28) hs_200000__pct,hs_25000_to_34999_pct,hs_Less_than_10000_pct

```{r}
pairs.panels(model_1_clean[,c(91,31:40)],  
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```

(#29~38) hs_200000__pct,hs_25000_to_34999_pct,hs_Less_than_10000_pct

```{r}
pairs.panels(model_1_clean[,c(91,41:50)],  
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```

```{r}
pairs.panels(model_1_clean[,c(1,41:50)],   
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE, 
             ellipses = FALSE 
             )
```

(#39~48)

```{r}
pairs.panels(model_1_clean[,c(91,51:60)],  
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE, 
             ellipses = FALSE 
             )
```

(#49~58) pct_all_people_under18_below_poverty

```{r}
pairs.panels(model_1_clean[,c(91,61:70)],  
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```

(#59~68) Production_transportation_and_material_moving_occupations_pct
Professional_scientific_and_management_and_administrative_and_waste_management_services_pct
Sales_and_office_occupations_pct


```{r}
pairs.panels(model_1_clean[,c(91,71:80)],   
             method = "pearson",
             hist.col = "#00AFBB",
             density = FALSE,  # show correlation ellipses
             )
```
(#69~78) 

```{r}
pairs.panels(model_1_clean[,c(91,81:89)],  
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```

2. After initially picked 34 census features having correlation with target over 0.1, checked again correlation between features and excluded highly correlated one (pick one)

 
```{r}

cols <- c('drove_alone_pct','Earning_mean','Earning_median_fmale_wk','Earning_median_male_wk','Earning_median_wk','females_employed_rate','hs_10000_to_14999_pct','hs_100000_to_149999_pct','hs_15000_to_24999_pct','hs_150000_to_194999_pct','hs_200000__pct','hs_25000_to_34999_pct','hs_Less_than_10000_pct','Income_mean','Income_median','labor_forces_rate','management_business_science_arts_occupations_pct','mean_travel_time','pct_all_families_below_poverty','pct_all_families_related_children_below18_below_poverty','pct_all_people_18plus_below_poverty','pct_all_people_18to64_below_poverty','pct_all_people_below_poverty','pct_all_people_related_children_under18_below_poverty','pct_all_people_related_children_under18_below5_below_poverty','pct_all_people_under18_below_poverty','Production_transportation_and_material_moving_occupations_pct','Professional_scientific_and_management_and_administrative_and_waste_management_services_pct','Sales_and_office_occupations_pct','service_occupations_pct','Social_Security_income_mean','Transportation_and_warehousing_and_utilities_pct','unemployment_rate','worked_from_home_pct'
)

temp <- select(model_1_clean,cols)
```

And identify summary stats of them.

```{r}
#colSums(is.na(temp))
summary(temp)
```
(Temp) Summary stat (total data)
```{r}

cols <- c('drove_alone_pct','Earning_mean','Earning_median_fmale_wk','Earning_median_male_wk','Earning_median_wk','females_employed_rate','hs_10000_to_14999_pct','hs_100000_to_149999_pct','hs_15000_to_24999_pct','hs_150000_to_194999_pct','hs_200000__pct','hs_25000_to_34999_pct','hs_Less_than_10000_pct','Income_mean','Income_median','labor_forces_rate','management_business_science_arts_occupations_pct','mean_travel_time','pct_all_families_below_poverty','pct_all_families_related_children_below18_below_poverty','pct_all_people_18plus_below_poverty','pct_all_people_18to64_below_poverty','pct_all_people_below_poverty','pct_all_people_related_children_under18_below_poverty','pct_all_people_related_children_under18_below5_below_poverty','pct_all_people_under18_below_poverty','Production_transportation_and_material_moving_occupations_pct','Professional_scientific_and_management_and_administrative_and_waste_management_services_pct','Sales_and_office_occupations_pct','service_occupations_pct','Social_Security_income_mean','Transportation_and_warehousing_and_utilities_pct','unemployment_rate','worked_from_home_pct'
)

temp <- select(model_2_clean,cols)
```

```{r}
knitr::kable(summary(temp), format = "markdown", caption="Overall summary statistics of Census")
```

```{r}

cols <- c('DISTRICT','Date_cat','Day_cat_2','Period_cat_num_2','Season','Month','SHIFT')

temp <- select(model_2_clean,cols)
```


```{r}
knitr::kable(summary(temp), format = "markdown", caption="Overall summary statistics of Origin.features")
```


There are some NAs but not much amount, so converted that with average value of each. 

```{r}
temp <- temp %>%
               mutate_all(~ifelse(is.na(.), mean(., na.rm = TRUE), .))
summary(temp)

```

```{r}


temp_1= temp[,c(1:7)]
cor_cs= cor(temp_1)
corrplot.mixed(cor_cs)

```

Finally picked 18 features as an input.


```{r, results='hide'}

cor_matrix <- cor(temp)
high_cor_pairs <- which(cor_matrix > 0.8 & cor_matrix != 1, arr.ind = TRUE)
high_cor_vars <- unique(c(high_cor_pairs[,1], high_cor_pairs[,2]))
high_cor_matrix <- cor_matrix[high_cor_vars, high_cor_vars]
high_cor_matrix
#corrplot(high_cor_matrix, method = "number")

```

Similary with categorical vars, try to find statistical clue by t-test.
Test result shows significance. by target group (compare mean of x by target groups)

Result for Income_mean var.

```{r}
t0 = subset(model_1_clean,Target_Clearance_time_f=='0')
t1 = subset(model_1_clean,Target_Clearance_time_f=='1')
agettest <- t.test(t0$Income_mean, t1$Income_mean)
agettest
```

**Histgogram of Numeric Variable w Target variable ** 

Following is histogram of `Earning_mean`(Census) by the target_Clearance_time groups (1: High, 0: Low)


```{r}

ggplot(model_1_clean, aes(Earning_mean, fill = Target_Clearance_time_f)) +  geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity')
ggplot(model_1_clean, aes(Earning_mean, fill = Target_Clearance_time_f)) + geom_density(alpha = 0.2)

```

Following is histogram of `worked_from_home_pct`(Census) by the target_Clearance_time groups (1: High, 0: Low)


```{r}

ggplot(model_1_clean, aes(worked_from_home_pct, fill = Target_Clearance_time_f)) +  geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity')
ggplot(model_1_clean, aes(worked_from_home_pct, fill = Target_Clearance_time_f)) + geom_density(alpha = 0.2)

```

**2. Build model **

Define evaluation functions.

```{r}

ConfusionMatrixInfo <- function( data, predict, actual, cutoff )
{	
	# extract the column ;
	# relevel making 1 appears on the more commonly seen position in 
	# a two by two confusion matrix	
	predict <- data[[predict]]
	actual  <- relevel( as.factor( data[[actual]] ), "1" )
	
	result <- data.table( actual = actual, predict = predict )

	# calculating each pred falls into which category for the confusion matrix
	result[ , type := ifelse( predict >= cutoff & actual == 1, "TP",
					  ifelse( predict >= cutoff & actual == 0, "FP", 
					  ifelse( predict <  cutoff & actual == 1, "FN", "TN" ) ) ) %>% as.factor() ]

	# jittering : can spread the points along the x axis 
	plot <- ggplot( result, aes( actual, predict, color = type ) ) + 
			geom_violin( fill = "white", color = NA ) +
			geom_jitter( shape = 1 ) + 
			geom_hline( yintercept = cutoff, color = "blue", alpha = 0.6 ) + 
			scale_y_continuous( limits = c( 0, 1 ) ) + 
			scale_color_discrete( breaks = c( "TP", "FN", "FP", "TN" ) ) + # ordering of the legend 
			guides( col = guide_legend( nrow = 2 ) ) + # adjust the legend to have two rows  
			ggtitle( sprintf( "Confusion Matrix with Cutoff at %.2f", cutoff ) )

	return( list( data = result, plot = plot ) )
}

proc_auc2 <- function(data, outcome_var, predictor_var) {
    pROC::auc(pROC::roc_(data, outcome_var, predictor_var))
}
```

**A. The Train and Test data are: **

```{r}
model_1_clean <- 
  readr::read_rds("Robbery_clean_p2_model_1.Rds")
```

```{r}
count(model_1_clean,Target_Clearance_time_f)
```

Final treatment before fitting. select features and convert NaN of Census.

```{r}
feat <- c('DISTRICT','Date_cat','Day_cat_2','Period_cat_num_2','Season','SHIFT','METHOD','drove_alone_pct','females_employed_rate','hs_10000_to_14999_pct','hs_100000_to_149999_pct','hs_15000_to_24999_pct','hs_150000_to_194999_pct','hs_200000__pct','hs_25000_to_34999_pct','hs_Less_than_10000_pct','Income_mean','labor_forces_rate','management_business_science_arts_occupations_pct','mean_travel_time','service_occupations_pct','Social_Security_income_mean','Transportation_and_warehousing_and_utilities_pct','unemployment_rate','worked_from_home_pct','Target_Clearance_time_f')

model_1_clean <- select(model_1_clean,feat)
```

```{r}
model_1_clean <- model_1_clean %>%
  mutate_at(vars(-Target_Clearance_time_f), ~ifelse(is.na(.), mean(., na.rm = TRUE), .))
```

```{r}
#Use 70% of dataset as training set and remaining 30% as testing set
set.seed(4321)
sample <- sample(c(TRUE, FALSE), nrow(model_1_clean), replace=TRUE, prob=c(0.8,0.2))
data_train_model_1  <- model_1_clean[sample, ]
data_test_model_1   <- model_1_clean[!sample, ]

nrow(data_train_model_1)
nrow(data_test_model_1)

```

```{r}
count(data_train_model_1,Target_Clearance_time_f)
```


```{r}
count(data_test_model_1,Target_Clearance_time_f)
```


**B. Train the model**

Then, train the first logistic regression model, and exclude vars that are not significant in terms of P-value, and re-fit. Finally, 5 vars are chosen. 


```{r}
#exclude vars that p-value is not significant 

Logit_model_1 <- glm(Target_Clearance_time_f ~ Date_cat +Day_cat_2 +DISTRICT +Period_cat_num_2 +Season +SHIFT +METHOD +drove_alone_pct +females_employed_rate +hs_10000_to_14999_pct +hs_100000_to_149999_pct +hs_15000_to_24999_pct +hs_150000_to_194999_pct +hs_200000__pct +hs_25000_to_34999_pct +hs_Less_than_10000_pct +Income_mean +labor_forces_rate +management_business_science_arts_occupations_pct +mean_travel_time +service_occupations_pct +Social_Security_income_mean +Transportation_and_warehousing_and_utilities_pct +unemployment_rate +worked_from_home_pct, data = data_train_model_1, family = "binomial")

#Target_clr_gun
```

Changed the low p-value features and refit.(But not picked them as performance decreased)

```{r}
#exclude vars that p-value is not significant 

#Logit_model_1 <- glm(Target_Clearance_time_f ~ Date_cat +DISTRICT +Period_cat_num_2 +SHIFT +METHOD +hs_10000_to_14999_pct +hs_100000_to_149999_pct +Income_mean +labor_forces_rate +management_business_science_arts_occupations_pct +mean_travel_time +Transportation_and_warehousing_and_utilities_pct +unemployment_rate +worked_from_home_pct, data = data_train_model_1, family = "binomial")

#Target_clr_gun
```


```{r}
summary(Logit_model_1)
Logit_model_sum <- summary(Logit_model_1)
```

```{r}
# prediction of p
data_train_model_1$prediction <- predict( Logit_model_1, newdata = data_train_model_1, type = "response" )
data_test_model_1$prediction  <- predict( Logit_model_1, newdata = data_test_model_1 , type = "response" )

```

** Quickly evaluate AUROC of the model

```{r}
h_train <- roc(Target_Clearance_time_f~prediction, data=data_train_model_1)
h_test <- roc(Target_Clearance_time_f~prediction, data=data_test_model_1)

plot(h_train)
plot(h_test)
```

Regardless of a cutoff level, the AUROC(area-under-curve) is 0.642, which is less than 0.8. The model does not seem to make the best good predict power.

```{r}
proc_auc2(data_test_model_1, 'Target_Clearance_time_f', 'prediction')
proc_auc2(data_train_model_1, 'Target_Clearance_time_f', 'prediction')

```

**Interpret (TBD)** 
Lockdown make the robbery difficult (long time to clear), Over>1, Lock(1) increase prob. high_diff_rob

```{r}
expcoeff = exp(coef(Logit_model_1))
expcoeff
#xkabledply( as.table(expcoeff), title = "Exponential of coefficients in model_1" )
```


**C. Evaluate the model**

C-1. Cut-off
By seeing probability distribution charts by target groups, cut-off (Criterion to preict=1) was set to 0.2.


```{r}
# distribution of the prediction score grouped by known outcome
ggplot( data_train_model_1, aes( prediction, color = Target_Clearance_time_f ) ) + geom_density( size = 1 ) +
ggtitle( "Train Set's Predicted Score(model_1)" ) + 
scale_color_economist( name = "data", labels = c( "Target(0)-short time", "Target(1)-longer time" ) ) + 
theme_economist()

```


```{r}
# distribution of the prediction score grouped by known outcome
ggplot( data_test_model_1, aes( prediction, color = Target_Clearance_time_f ) ) + geom_density( size = 1 ) +
ggtitle( "Test Set's Predicted Score(model_1)" ) + 
scale_color_economist( name = "data", labels = c( "Target(0)-short time", "Target(1)-longer time" ) ) + 
theme_economist()
```

C-2 Accuracy, Recall

```{r}
# visualize .4 cutoff (lowest point of the previous plot)

cm_info <- ConfusionMatrixInfo( data = data_test_model_1, predict = "prediction", 
                                actual = "Target_Clearance_time_f", cutoff = .5 )

#ggthemr("flat")

cm_info$plot
#print(cm_info$data)

```

```{r}
temp <- cm_info$data
TP <- nrow(subset(temp,subset = temp$type == 'TP'))
TN <- nrow(subset(temp,subset = temp$type == 'TN'))
FP <- nrow(subset(temp,subset = temp$type == 'FP'))
FN <- nrow(subset(temp,subset = temp$type == 'FN'))

Total <-nrow(data_test_model_1)

```


```{r}
Accuracy <-  (TP + TN) / Total
Precision  <- TP / (TP + FP)
Recall_rate <- TP / (TP + FN)
Specificity <- TN / (TN + FP)
F1 <- 2 *(Precision)*(Recall_rate)/(Precision + Recall_rate)
FPR <- FP/(TN+FP)
FNR <- FN/(TP+FN)

Accuracy
Precision
Recall_rate
F1
FPR
FNR
```
```{r}
count(temp,actual)
```


Evaluate per target levels.

```{r}
temp <- cm_info$data
df_neg <- subset(temp,temp$actual == 0) 
df_pos <- subset(temp,temp$actual == 1) 
```

Performance of Negtive (target=0, Low time)
```{r}
TP <- nrow(subset(df_neg,subset = df_neg$type == 'TP'))
TN <- nrow(subset(df_neg,subset = df_neg$type == 'TN'))
FP <- nrow(subset(df_neg,subset = df_neg$type == 'FP'))
FN <- nrow(subset(df_neg,subset = df_neg$type == 'FN'))

Total <-nrow(subset(data_test_model_1,data_test_model_1$Target_Clearance_time_f == 0))

Accuracy <-  (TP + TN) / Total
Precision  <- TP / (TP + FP)
Recall_rate <- TP / (TP + FN)
Specificity <- TN / (TN + FP)
F1 <- 2 *(Precision)*(Recall_rate)/(Precision + Recall_rate)
FPR <- FP/(TN+FP)
FNR <- FN/(TP+FN)

Accuracy
Precision
Recall_rate
F1
FPR
FNR
```


Performance of Positive (target=1, High time)
```{r}
TP <- nrow(subset(df_pos,subset = df_pos$type == 'TP'))
TN <- nrow(subset(df_pos,subset = df_pos$type == 'TN'))
FP <- nrow(subset(df_pos,subset = df_pos$type == 'FP'))
FN <- nrow(subset(df_pos,subset = df_pos$type == 'FN'))

Total <-nrow(subset(data_test_model_1,data_test_model_1$Target_Clearance_time_f == 1))

Accuracy <-  (TP + TN) / Total
Precision  <- TP / (TP + FP)
Recall_rate <- TP / (TP + FN)
Specificity <- TN / (TN + FP)
F1 <- 2 *(Precision)*(Recall_rate)/(Precision + Recall_rate)
FPR <- FP/(TN+FP)
FNR <- FN/(TP+FN)

Accuracy
Precision
Recall_rate
F1
FPR
FNR
```

### 4. Model 2- Logistic regression (`Target_robbery_method`)

Data: model_2_clean
Target: `Target_robbery_method` 

```{r}
model_2_clean <- 
  readr::read_rds("Robbery_clean_p2_model_2.Rds")
```

```{r}
count(model_2_clean,Target_robbery_method)
```


```{r}
str(model_2_clean)
```


**1. Feature Selection **


**A. Correlation- original variables (w.o Census)**

```{r}
library(psych)
pairs.panels(model_2_clean[,c(1:10)], 
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```

**B. Correlation- Census variables **
```{r}
pairs.panels(model_2_clean[,c(1, 11:20)], 
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```


```{r}
pairs.panels(model_2_clean[,c(1, 21:30)], 
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```


```{r}
pairs.panels(model_2_clean[,c(1,31:40)], 
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```


```{r}
pairs.panels(model_2_clean[,c(1,41:50)], 
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```


```{r}
pairs.panels(model_2_clean[,c(1,51:60)], 
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```



```{r}
pairs.panels(model_2_clean[,c(1,61:70)], 
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```



```{r}
pairs.panels(model_2_clean[,c(1,71:80)], 
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```



```{r}
pairs.panels(model_2_clean[,c(1,80:86)], 
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```

**Final variables chosen: DISTRICT + Period_cat_num_2 + Season + Month +SHIFT + drove_alone_pct + Earning_mean + Income_mean + labor_forces_rate + mean_travel_time + unemployment_rate **

**Check the final version correlation **
```{r}
pairs.panels(model_2_clean[,c("Target_robbery_method","DISTRICT","Period_cat_num_2","Season", "Month", "SHIFT", "drove_alone_pct", "Earning_mean", "Income_mean", "labor_forces_rate",  "mean_travel_time", "unemployment_rate")], 
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```

**C. Mini EDA for candidate features**

Features that are used in final model
 1.Origin features
 2. 
DISTRICT + 
Period_cat_num_2 + 
Season + Month +SHIFT 
+ drove_alone_pct
 + Earning_mean 
+ Income_mean
 + labor_forces_rate 
+ mean_travel_time 
+ unemployment_rate 
+ walked_pct + Sales_and_office_occupations_pct 
+ public_transportation_pct



**1. Categorical Variable w Target variable ** 

DISTRICT.

```{r}
temp <-  model_2_clean %>% group_by(DISTRICT) %>%
                   summarise(avg = mean(as.numeric(Target_robbery_method), na.rm = TRUE),.groups = 'drop') 

arrange(temp, DISTRICT) 
temp_gr <- data.frame(temp)

plot <- ggplot(temp_gr, aes(DISTRICT, avg)) + geom_line() + labs(x="district", y="Target_robbery_method(% of GUN)") +
  labs(title="Prop. of Gun method by district") + scale_x_discrete(limits=c("1","2","3","4","5","6","7")) + geom_point() 

plot
```


**2. Numeric Variable w Target variable ** 

Following is histogram of `Earning_mean`(Census) by the target_Clearance_time groups (1: High, 0: Low)


```{r}

ggplot(model_2_clean, aes(drove_alone_pct
, fill = Target_robbery_method)) +  geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity')
ggplot(model_2_clean, aes(drove_alone_pct
, fill = Target_robbery_method)) + geom_density(alpha = 0.2)

```

**2. Build model **

**A. The Train and Test data are: **

```{r}
set.seed(8343)
sample <- sample(c(TRUE, FALSE), nrow(model_2_clean), replace=TRUE, prob=c(0.75,0.25))
df_train <- model_2_clean[sample, ]
df_test  <- model_2_clean[!sample, ]

nrow(df_train)
nrow(df_test )
```

**B. Train the model**
```{r}
Model <- glm(Target_robbery_method ~ DISTRICT + +Period_cat_num_2 + 
    Season + Month + SHIFT + drove_alone_pct + mean_travel_time + 
    unemployment_rate + walked_pct + Sales_and_office_occupations_pct + 
    public_transportation_pct, data = df_train, family = "binomial")

sum_model = summary(Model)
sum_model
```

```{r}
expcoeff = exp(coef(Model))
expcoeff
```
**C. Evaluate the model**
```{r}
df_train$prediction <- predict( Model, newdata = df_train, type = "response" )
df_test$prediction  <- predict( Model, newdata = df_test , type = "response" )
```


```{r}
ggplot( df_train, aes( prediction, color = Target_robbery_method ) ) + geom_density( size = 1 ) +
ggtitle( "Train Set's Predicted Score" ) 
```

```{r}
ggplot( df_test, aes( prediction, color = Target_robbery_method ) ) + geom_density( size = 1 ) +
ggtitle( "Test Set's Predicted Score" ) 
```


```{r}
h <- roc(Target_robbery_method~prediction, data=df_train)
plot(h)
auc(h)
```
final version: ROC 65.9 (final)

```{r}
h <- roc(Target_robbery_method~prediction, data=df_test)
plot(h)
auc(h)
```

Accurcy, Precision, Recall when cut-off 0.5

```{r}
cm_info <- ConfusionMatrixInfo( data = df_test, predict = "prediction", 
                                actual = "Target_robbery_method", cutoff = .5 )

cm_info$plot
```

```{r}
temp <- cm_info$data
TP <- nrow(subset(temp,subset = temp$type == 'TP'))
TN <- nrow(subset(temp,subset = temp$type == 'TN'))
FP <- nrow(subset(temp,subset = temp$type == 'FP'))
FN <- nrow(subset(temp,subset = temp$type == 'FN'))

Total <-nrow(df_test)
Accuracy <-  (TP + TN) / Total
Precision  <- TP / (TP + FP)
Recall_rate <- TP / (TP + FN)
Specificity <- TN / (TN + FP)
F1 <- 2 *(Precision)*(Recall_rate)/(Precision + Recall_rate)
FPR <- FP/(TN+FP)
FNR <- FN/(TP+FN)

Accuracy
Precision
Recall_rate
F1
FPR
FNR
```

```{r}
temp <- cm_info$data
df_neg <- subset(temp,temp$actual == 0) 
df_pos <- subset(temp,temp$actual == 1) 
```

Performance of Negtive (target=0, Low time)
```{r}
TP <- nrow(subset(df_neg,subset = df_neg$type == 'TP'))
TN <- nrow(subset(df_neg,subset = df_neg$type == 'TN'))
FP <- nrow(subset(df_neg,subset = df_neg$type == 'FP'))
FN <- nrow(subset(df_neg,subset = df_neg$type == 'FN'))

Total <-nrow(subset(df_test,df_test$Target_robbery_method == 0))

Accuracy <-  (TP + TN) / Total
Precision  <- TP / (TP + FP)
Recall_rate <- TP / (TP + FN)
Specificity <- TN / (TN + FP)
F1 <- 2 *(Precision)*(Recall_rate)/(Precision + Recall_rate)
FPR <- FP/(TN+FP)
FNR <- FN/(TP+FN)

Accuracy
Precision
Recall_rate
F1
FPR
FNR
```

Performance of Positive (target=1, High time)
```{r}
TP <- nrow(subset(df_pos,subset = df_pos$type == 'TP'))
TN <- nrow(subset(df_pos,subset = df_pos$type == 'TN'))
FP <- nrow(subset(df_pos,subset = df_pos$type == 'FP'))
FN <- nrow(subset(df_pos,subset = df_pos$type == 'FN'))

Total <-nrow(subset(df_test,df_test$Target_robbery_method == 1))

Accuracy <-  (TP + TN) / Total
Precision  <- TP / (TP + FP)
Recall_rate <- TP / (TP + FN)
Specificity <- TN / (TN + FP)
F1 <- 2 *(Precision)*(Recall_rate)/(Precision + Recall_rate)
FPR <- FP/(TN+FP)
FNR <- FN/(TP+FN)

Accuracy
Precision
Recall_rate
F1
FPR
FNR
```

**3. Model 2-2 (Decision Tree with model_2_clean): Predict `Target_robbery_method`. **

**Load cleaned data again  **

```{r}

model_2_clean <- 
  readr::read_rds("Robbery_clean_p2_model_2.Rds")

```


**2. Build model **

**A. The Train and Test data are: **

```{r}
#Use 70% of dataset as training set and remaining 30% as testing set
set.seed(4321)
sample <- sample(c(TRUE, FALSE), nrow(model_2_clean), replace=TRUE, prob=c(0.6,0.4))
data_train_model_1  <- model_2_clean[sample, ]
data_test_model_1   <- model_2_clean[!sample, ]

nrow(data_train_model_1)
nrow(data_test_model_1)

```

**B. Train the model**

Use same variables with logistic regression.

```{r}
set.seed(4321)

Tree_model_gun <- rpart(Target_robbery_method ~ DISTRICT + + Period_cat_num_2 + Season + Month +SHIFT + drove_alone_pct + Earning_mean + Income_mean + labor_forces_rate + mean_travel_time + unemployment_rate + walked_pct + Sales_and_office_occupations_pct + public_transportation_pct, data=data_train_model_1, method="class", control = list(maxdepth = 30))
```

Check if tree is pruned.

```{r}
# check if rpart object is a tree
if (Tree_model_gun$frame$var[1] != "<leaf>") {
  # plot tree 
  plot(Tree_model_gun, uniform=TRUE, main="Classification Tree for Target_robbery_method")
} else {
  # print message if rpart object is not a tree
  print("The rpart object is not a tree.")
}
```


```{r}

printcp(Tree_model_gun) # display the results 
plotcp(Tree_model_gun) # visualize cross-validation results 
summary(Tree_model_gun) # detailed summary of splits

# plot tree 
plot(Tree_model_gun, uniform=TRUE, main="Classification Tree for Target_robbery_method")
text(Tree_model_gun, use.n=TRUE, all=TRUE, cex=.8)
```

**B. Visualize the tree model**

Next, we can try two other ways to plot the tree, with library `rpart.plot` and a "fancy" plot using the library `rattle`.

```{r fancyplot}
loadPkg("rpart.plot")
loadPkg("rattle") # For fancyRpartPlot (Trees) Answer "no" on installing from binary source
```

```{r}
rpart.plot(Tree_model_gun)
fancyRpartPlot(Tree_model_gun)

```

This is the prediction of the model, probability `p`.


```{r}
# Predict probabilities on the test set
predictions  <- predict(Tree_model_gun, newdata = data_test_model_1, type = "prob")
# Extract the probability of the positive class
data_test_model_1$prediction <- predictions[, 2]
```

```{r}
predictions  <- predict(Tree_model_gun, newdata = data_train_model_1, type = "prob")
# Extract the probability of the positive class
data_train_model_1$prediction <- predictions[, 2]
```


C-1. Cut-off
By seeing probability distribution charts by target groups, cut-off (Criterion to preict=1) was set to 0.2.


```{r}
# distribution of the prediction score grouped by known outcome
ggplot( data_train_model_1, aes( prediction, color = Target_robbery_method ) ) + geom_density( size = 1 ) +
ggtitle( "Train Set's Predicted Score" ) + 
scale_color_economist( name = "data", labels = c( "Lower time", "Higher time" ) ) + 
theme_economist()

```

```{r}
# distribution of the prediction score grouped by known outcome
ggplot( data_test_model_1, aes( prediction, color = Target_robbery_method ) ) + geom_density( size = 1 ) +
ggtitle( "Test Set's Predicted Score" ) + 
scale_color_economist( name = "data", labels = c( "Lower time", "Higher time" ) ) + 
theme_economist()
```

C-2 Accuracy, Recall

```{r}
# visualize .4 cutoff (lowest point of the previous plot)

cm_info <- ConfusionMatrixInfo( data = data_test_model_1, predict = "prediction", 
                                actual = "Target_robbery_method", cutoff = .5 )

#ggthemr("flat")

cm_info$plot
#print(cm_info$data)

```

```{r}
temp <- cm_info$data
TP <- nrow(subset(temp,subset = temp$type == 'TP'))
TN <- nrow(subset(temp,subset = temp$type == 'TN'))
FP <- nrow(subset(temp,subset = temp$type == 'FP'))
FN <- nrow(subset(temp,subset = temp$type == 'FN'))

Total <-nrow(data_test_model_1)
```

```{r}
Accuracy <-  (TP + TN) / Total
Precision  <- TP / (TP + FP)
Recall_rate <- TP / (TP + FN)
Specificity <- TN / (TN + FP)
F1 <- 2 *(Precision)*(Recall_rate)/(Precision + Recall_rate)
FPR <- FP/(TN+FP)
FNR <- FN/(TP+FN)

Accuracy
Precision
Recall_rate
F1
FPR
FNR
```


C-3. AUROC

```{r}
h <- roc(Target_robbery_method~prediction, data=data_test_model_1)

plot(h)
```

```{r}
proc_auc2(data_test_model_1, 'Target_robbery_method', 'prediction')
proc_auc2(data_train_model_1, 'Target_robbery_method', 'prediction')

```


## Analysis 

### 1. 

## Conclusion

## References 
