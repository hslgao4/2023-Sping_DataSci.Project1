---
title: "Analysis of Robbery Rates in Washington D.C: Identifying Factors that Influence Police Efficiency and Severity of Robbery"
author: "Muhannad Alwhebie, Liang Gao, Nammin Woo"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=F}
library(ezids)
library(readr) 
library(ggplot2)
library(DBI) 
library(magrittr)
library(dplyr)  
library(tidyr) 
library(tidyverse)
library(graphics) 
library(gridExtra) 
library(data.table)
library(ROCR)
library(grid)
library(broom)
library(scales)
library(ggthemes)
library(psych)
loadPkg("pROC") 
loadPkg("rpart")
library(corrplot)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
options(scipen=9999, digits = 3) 
```



## I. Introduction 

This project aims to build upon the previous exploratory data analysis project that examined the impact of COVID-19 lockdown on robbery rates in Washington D.C. In this project, our objective is to identify factors that influence police efficiency and severity of robbery crime. To gain a more comprehensive understanding of the underlying factors affecting robbery rates, we plan to incorporate demographic and economic data from the Census Bureau, such as median household income, poverty rate, and education level, as predictor variables in our model.

We will use two target variables in our analysis: Target_Clearance_time (whether the robbery took longer to clear) and Target_robbery_method (whether the robbery involved a gun or another type of weapon). By analyzing the relationships between these target variables and other explanatory variables, we hope to gain insights into the factors that affect clearance time and the use of guns in robberies.

The previous project found changes in robbery frequency during the pandemic, with underlying socio-economic factors potentially contributing to the patterns. Additionally, it supported the findings of the Boman and Gallupe (2020) study that violent crime increased during the lockdown period, with a dramatic increase in the number of robberies during Autumn. By utilizing GLM models to answer SMART questions related to robbery rates, we aim to gain a deeper understanding of the underlying factors that affect robbery rates in Washington D.C and provide insights that can inform future policy decisions related to crime prevention and public safety.

**Goal of this project: Building models to identify factors that influence police efficiency and severity of robbery.**



**Our SMART questions for this project include:**

1. What are the most important features that influence the clearance time and severity of robbery methods?

2. How accurately can we predict the clearance time and method based on these features?


## II. Methodology


**Data Sourcing**:

The dataset used in this study was sourced from the Open Data DC repository, which contains roughly 8000 observations of robbery incidents in the District of Columbia between January 2019 and December 2022. In addition, we obtained the Economic Characteristics of DC Census Tracts surveyed by the U.S. government using the CENSUS_TRACT variable.

**Data Pre-processing:**

After obtaining the necessary data, we first aggregated the census bureau data to the original dataset to include economic characteristics. Next, we defined two target variables, Target_Clearance_time and Target_robbery_method, which represent whether the robbery took longer to clear and whether a gun was used or not, respectively. We then conducted exploratory data analysis (EDA) to gain insights into the data and performed pre-processing to prepare the data for predictive modeling.

**Feature Selection:**


We used correlation analysis to identify the features that have the strongest relationship with the target variables. We also performed EDA on the target variables to understand their distribution and behavior with respect to the candidate features.



Overall, the methodology used in this study involved data sourcing, preprocessing and feature selection. The inclusion of economic characteristics data and the use of machine learning techniques can help to provide insights into the underlying factors that influence robbery rates in Washington D.C.


### Ⅰ. Data sourcing

We initially used the latest data in project 1, Robbery_clean. The outliers of clearance_rate have been removed (converted into NA).

```{r}

# the latest data of Project 1
#Robbery <- 
#  readr::read_rds("Robbery_230303.Rds")

# the latest data of Project 1 (The outliers of clearance_rate have been removed (converted into NA))
#Robbery_clean <- 
#  readr::read_rds("Robbery_clean_230303.Rds")


#Import data of project 2: started from Robbery_clean (added CENSUS_TRACT for aggregating census feature)
Robbery_clean_p2 = data.frame(read.csv("Robbery_clean_p2.csv"))
#Robbery_p2 = data.frame(read.csv("Robbery_p2.csv"))  #Data before cleansing outliers
Census_eco = data.frame(read.csv("ACS_Economic_Census_clean.csv"))

```


```{r}
#head(Robbery_clean_p2,5) 
xkabledply(Robbery_clean_p2[1:4,], title = "First Four Rows of initial main Data (Robbery)")
```

Following 10 original features were mainly used as predictors in this project 2.

* `Period_cat_num`: a categorical variable indicating whether the robbery occurred before, during, or after the COVID-19 lockdown period

* `Month`: the month the robbery incident occurred (Transformed: Month_2, Month_3)
* `Season`: the season in which the robbery incident occurred
* `Date_cat`: a categorical variable indicating whether the robbery incident occurred at the beginning, middle, or end of the month
* `SHIFT`: a categorical variable indicating the shift during which the robbery incident occurred, which can be "DAY," "EVENING," or "NIGHT"
* `Day_cat`: the day of the week the robbery incident occurred, represented as an integer
* `Day_cat_2`: a categorical variable indicating whether the robbery incident occurred on a weekday or weekend
* `METHOD`: a categorical variable indicating the method used in the robbery, which can be "GUN," "KNIFE," or "OTHER"
* `DISTRICT`: the police district where the robbery incident occurred, represented as an integer
* `Clearance_rate`: the percentage of robbery incidents that were cleared by the police

And to help with predictive modeling, we decided to add a few new features that come from the Census. 
This is census Tracts data that we will add on to original dataset.


```{r}
#head(Robbery_clean_p2,5) 
xkabledply(Census_eco[1:4,], title = "First Four Rows of Census Tracts Dataframe")
```


All robbery records in our dataset have its own location information that the robberies are occurred.
We already used location variable `DISTRICT` in project 1 meaning Police Service Area, and additionally used location variable `CENSUS_TRACT` meaning statistical subdivisions of a county.(Population from 1,200 to 1,800)

Specifically, we used the **specific Economic Characteristics of DC Census Tracts** that are surveyed from U.S government instead of directly using `CENSUS_TRACT`.

Economic Characteristics are about Employment, Commuting, Occupation, Income, Health Insurance, Poverty,etc., and it is based on the most current release of data from the American Community Survey (ACS) about economic characteristics. (2020 Census Tract boundaries). 

All Census features were Wrangled through follows:  ​
  - Checked basic things of each feature (existence of NaN, distribution,etc.)​
  - Name of features were changed for being easily recognized for analysis.​
  - Considering purpose of analysis, all features are re-scaled as proportion and dummified (ex. percentage of workers who are using carpool for commuting) or summarized (Mean, Median).
  

```{r, results='hide'}
#columns that are necessary to anaysis
#ex_cols <- c('OBJECTID','STATEFP','COUNTYFP','CENSUS_TRACT','GEOID')
#temp <- select(Census_eco,-OBJECTID,-STATEFP,-COUNTYFP,-CENSUS_TRACT,-GEOID)
#knitr::kable(temp, format = "markdown", caption="Overall summary statistics of DC Census Tract(Economic Characteristics)")
```

Census features were aggregated to initial main data by the key of `CENSUS_TRACT`.

```{r}
#Add new features joining with key
temp <- select(Census_eco,-OBJECTID,-STATEFP,-COUNTYFP,-GEOID)
Robbery_clean_p2_add <- merge(x = Robbery_clean_p2, y = temp, by = "CENSUS_TRACT", all.x = TRUE)
```

The dataset consists of `r nrow(Robbery_clean_p2_add)` observations and `r ncol(Robbery_clean_p2_add)` variables. including `r ncol(Census_eco)-5` Census Economic Characteristics features.
Number of Census features by the categories are belows:
 + INCOME:	20
 + POVERTY: 19 
 + INDUSTRY: 12 
 + EMPLOYMENT: 9
 + COMMUTING TO WORK:	8
 + OCCUPATION: 5
 + CLASS OF WORKER: 4


```{r, results='hide'}
#count(Robbery_clean_p2,SHIFT)
#head(temp,5) 
nrow(Robbery_clean_p2)
nrow(Robbery_clean_p2_add)
ncol(Robbery_clean_p2) #Before aggregation
ncol(Census_eco)-5     #Census 
ncol(Robbery_clean_p2_add) #After aggregation(add Cesnsus)
```

### Ⅱ. Define target variables

The first target variable we will define is `Target_Clearance_time.` This variable will indicate whether a robbery took longer to clear or not. The clearance time will be defined as the time between the initial report of the robbery and the time when the case is closed. If the clearance time is longer than the median clearance time for all cases in the dataset, the value for `Target_Clearance_time` will be 1, indicating that the robbery took longer to clear. If the clearance time is shorter than the median, the value for `Target_Clearance_time` will be 0, indicating that the robbery did not take longer to clear.

The second target variable we will define is `Target_robbery_method.` This variable will indicate whether the robbery was committed with a gun or with some other weapon or no weapon at all. We will use the "Gun" variable from the dataset to determine the robbery method. If the weapon used was a gun, the value for Target_robbery_method will be 1, indicating that the robbery was committed with a gun. If the weapon used was anything else, the value for `Target_robbery_method` will be 0, indicating that the robbery was committed with some other weapon or no weapon at all.

By defining these two target variables, we can build predictive models that will allow us to identify the factors that contribute to longer clearance times and gun-related robberies. We can then use this information to develop strategies for reducing these types of crimes in Washington D.C.


```{r}
#Summary
#columns to analyse 
cols <- c('Clearance_rate','METHOD')
#summary(select(Robbery,cols))
#summary(select(Robbery_clean_p2_add,cols))   

knitr::kable(summary(select(Robbery_clean_p2_add,cols)), format = "markdown", caption="T1.Summary statistics of Target variables")

```


**1. `Target_Clearance_time` implies Efficiency of Police or Difficulty of incident, whether need Longer time to clear or not.**



```{r}
plot <- ggplot(data=Robbery_clean_p2_add, aes(Clearance_rate)) + 
  geom_histogram(col="darkgrey", 
                 fill="lightgrey", 
                 alpha = .7,bins = 30) + 
  labs(x="Clearance_rate", y="Frequency") +
  labs(title="C1.Clearance_rate frequency") 
plot
```


```{r}
#TWO var Groupby(mean of target by X category groups)
temp <-  Robbery_clean_p2_add %>% group_by(DISTRICT) %>%
                   summarise(avg = mean(Clearance_rate, na.rm = TRUE),.groups = 'drop')  #Targe(%): mean( as.numeric(Target_Clearance_time)

arrange(temp, DISTRICT) 
temp_gr <- data.frame(temp)

#Make chart (scatter)
plot <- ggplot(temp_gr, aes(DISTRICT, avg)) + geom_line() + labs(x="district", y="Avg.Clearance time(min)") +
  labs(title="C2.Avg.Clearance time by district") + scale_x_discrete(limits=c("1","2","3","4","5","6","7")) + geom_point() #with 3-D: ggplot(temp_gr, aes(x, y, colour = Period_cat))  

plot
```


Chart C1 shows `Clearance_rate` is highly right skewed, and Chart C2 displaying `avg.Clearance_time` per `DISTRICT` exhibits clearance time of robbery visually distinguished by the threshold 40~50 min(similar to average). So, We defined Long clearance time as over 40 min and short clearance time as less than 40 min.

```{r}

#Final target
Robbery_clean_p2_add$Target_Clearance_time_f <- ifelse(Robbery_clean_p2_add$Clearance_rate >= 40,
c(1), c(0))

#This one is optional (over Q3)
Robbery_clean_p2_add$Target_Clearance_time <- ifelse(Robbery_clean_p2_add$Clearance_rate >= 85,
c(1), c(0))

#This one is optional (GUN&Clear)
Robbery_clean_p2_add$Target_clr_gun <- ifelse((Robbery_clean_p2_add$Target_Clearance_time_f ==1) & (Robbery_clean_p2_add$METHOD =='GUN'), c(1),c(0))


count(Robbery_clean_p2_add,Target_Clearance_time_f, sort=FALSE)

```


Our first modeling target, `Target_Clearance_time` has three levels as follows:

  + `Target_Clearance_time`=1 : Need long time, more than 40 min
  + `Target_Clearance_time`=0 : Need short time, less than 40 min 
  +  NA: `Outliers` (Need extremely long time (avg 2.5 days).



**2. `Target_robbery_method` indicates Severity of robbery, whether firearm, Gun was used or not.**

```{r}
# 1
Robbery_clean_p2_add$Target_robbery_method <- ifelse(Robbery_clean_p2_add$METHOD == 'GUN',c(1), c(0))
count(Robbery_clean_p2_add,Target_robbery_method, sort=FALSE)
```

Our second modeling target, `Target_robbery_method` has two levels as follows:  

  + `Target_robbery_method`=1 : Gun was used for robbery.
  + `Target_robbery_method`=0 : Knife or Other weapons were used,


```{r}
#Temporary Saved current data.

#Dataframe Save and load
#readr::write_rds(Robbery_clean_p2_add, "Robbery_clean_p2_add.Rds")

#Can load the dataset of this stage by letting active below code 
#Robbery_clean_p2_add <- 
#  readr::read_rds("Robbery_clean_p2_add.Rds")
```


### Ⅲ. Pre-processing for predictive modeling

From this section, we will check only input/target variables that are used for modeling.

Check data type of original variables (With or Without Census Track)

**1. Revise/Add features**

 (A) In terms of using `Period_cat_num` to precis target, we simplified the period into two categories (Lock-down=1, other=0).

```{r}
attach(Robbery_clean_p2_add)
Robbery_clean_p2_add$Period_cat_num_2[Period_cat == 'Pre'] <- 0
Robbery_clean_p2_add$Period_cat_num_2[Period_cat == 'Lock'] <- 1
Robbery_clean_p2_add$Period_cat_num_2[Period_cat  == 'Post'] <- 0
detach(Robbery_clean_p2_add)
#summary(Robbery_clean_p2_add$Period_cat_num_2)
```


 (B) `Hour` Start time of police clearance was created.

```{r}
#colnames(Robbery_clean_p2_model)
library(lubridate)
#head(Robbery_clean_p2_add$START_DATE,5)
Robbery_clean_p2_add$Hour <- hour(Robbery_clean_p2_add$START_DATE)
```


```{r, results='hide'}
co = c('START_DATE','Hour')
head(select(Robbery_clean_p2_add,co),5)
```

Types of original predictors (w.o Census) are below. Some categorical features needs to be **converted into Factor**. for logistic regression.  

```{r}
col_org <- c('Date_cat','Day_cat','Day_cat_2','DISTRICT','Month','Period_cat_num_2','Season','SHIFT','Hour','METHOD')

str(select(Robbery_clean_p2_add,col_org)) 
#sum_list <- summary(Robbery_clean_p2_add)   
#write.csv(sum_list, "sum_list.csv", row.names=FALSE)
```

**2. Change data types of features**

 (A) So, a few categorical variables including targets were Changed into factor. 

```{r}
cols <- c('Date_cat','Day_cat_2','Season','SHIFT','METHOD','Period_cat_num_2','Target_Clearance_time_f','Target_robbery_method')  
#array
Robbery_clean_p2_add %<>%
       mutate_each_(funs(factor(.)),cols) 

str(select(Robbery_clean_p2_add,cols)) 
```

B. And a few Census features were converted into numeric.  


```{r}
#15 vars 
cols <- c('females_employed_rate','hs_10000_to_14999_pct','hs_100000_to_149999_pct','hs_15000_to_24999_pct','hs_150000_to_194999_pct','hs_200000__pct','hs_25000_to_34999_pct','hs_35000_to_49999_pct','hs_50000_to_74999_pct','hs_75000_to_99999_pct','hs_Less_than_10000_pct','hs_own_children_6_to_17_labor_force_pct','hs_own_children_6_to_17_pct','hs_own_children_under_6_labor_force_pct','hs_own_children_under_6_pct')  #array
Robbery_clean_p2_add[cols] <- sapply(Robbery_clean_p2_add[cols],as.numeric)
#str(select(Robbery_clean_p2_add,cols)) 
```


Finally, The dataset consists of `r ncol(Robbery_clean_p2_add)` variables. including `r ncol(Census_eco)-5` Census Economic Characteristics features.


```{r, results='hide'}
nrow(Robbery_clean_p2_add)    #Census 
ncol(Robbery_clean_p2_add) #After aggregation(add Cesnsus)
#str(Robbery_clean_p2_add)
```

```{r}
#Dataframe Save and load
#readr::write_rds(Robbery_clean_p2_add, "Robbery_clean_p2_model.Rds")

#Can load the dataset of this stage by letting active below code 
#Robbery_clean_p2_model <- 
#  readr::read_rds("Robbery_clean_p2_model.Rds")
```


**3-1.Finalize defining modeling data 2 for `Target_robbery_method`**  

```{r, results='hide'}
#count(Robbery_clean_p2_add,Target_robbery_method, sort=FALSE)

  cols_m2 <- c('Target_robbery_method','Hour','Date_cat','Day_cat','Day_cat_2','DISTRICT','Month','Period_cat_num_2','Season','SHIFT','Agriculture_forestry_fishing_and_hunting_and_mining_pct','armed_forces_rate','Arts_entertainment_and_recreation_and_accommodation_and_food_services_pct','carpooled_pct','Construction_pct','drove_alone_pct','Earning_mean','Earning_median_fmale_wk','Earning_median_male_wk','Earning_median_wk','Educational_services_and_health_care_and_social_assistance_pct','females_employed_rate','Finance_and_insurance_and_real_estate_and_rental_and_leasing_pct','Government_workers_pct','hs_10000_to_14999_pct','hs_100000_to_149999_pct','hs_15000_to_24999_pct','hs_150000_to_194999_pct','hs_200000__pct','hs_25000_to_34999_pct','hs_35000_to_49999_pct','hs_50000_to_74999_pct','hs_75000_to_99999_pct','hs_Less_than_10000_pct','hs_own_children_6_to_17_labor_force_pct','hs_own_children_6_to_17_pct','hs_own_children_under_6_labor_force_pct','hs_own_children_under_6_pct','Income_mean','Income_median','Information_pct','labor_forces_rate','management_business_science_arts_occupations_pct','Manufacturing_pct','mean_travel_time','Natural_resources_construction_and_maintenance_occupations_pct','other_means_pct','Other_services_except_public_administration_pct','pct_all_families_below_poverty','pct_all_families_related_children_below18_below_poverty','pct_all_families_related_children_below5_below_poverty','pct_all_people_18plus_below_poverty','pct_all_people_18to64_below_poverty','pct_all_people_65plus_below_poverty','pct_all_people_below_poverty','pct_all_people_related_children_under18_5to17_below_poverty','pct_all_people_related_children_under18_below_poverty','pct_all_people_related_children_under18_below5_below_poverty','pct_all_people_under18_below_poverty','pct_below_poverty_female_no_spouse_present','pct_below_poverty_female_no_spouse_present_related_children_below18','pct_female_no_spouse_present_related_children_below18_below5_below_poverty','pct_married_couple_families_below_poverty','pct_married_couple_families_related_children_below18_below_poverty','pct_married_couple_families_related_children_below5_below_poverty','pct_people_in_families_below_poverty','pct_unrelated_individuals_15plus_below_poverty','pop_16_over_emp_status','Private_wage_and_salary_workers_pct','Production_transportation_and_material_moving_occupations_pct','Professional_scientific_and_management_and_administrative_and_waste_management_services_pct','Public_administration_pct','public_assistance_income_mean','public_transportation_pct','Retail_trade_pct','Retirement_income_mean','Sales_and_office_occupations_pct','Self_employed_in_own_not_incorporated_business_workers_pct','service_occupations_pct','Social_Security_income_mean','Supplemental_Security_income_mean','Transportation_and_warehousing_and_utilities_pct','unemployment_rate','Unpaid_family_workers_pct','walked_pct','Wholesale_trade_pct','worked_from_home_pct','workers_16_over')

model_2_clean <- select(Robbery_clean_p2_add,cols_m2)
#str(model_2_clean)
```

To finalize modeling data 2 for `Target_robbery_method`, 

 + The dataset has `r nrow(model_2_clean)` observations same as the initial dataset,
 + and `r ncol(model_2_clean)-1` features and 1 target variable after excluding needless columns for modeling. 


```{r, results='hide'}
nrow(model_2_clean)    #Census 
ncol(model_2_clean) #After aggregation(add Cesnsus)
#str(model_2_clean)
```

```{r}
#Dataframe Save and load
#readr::write_rds(model_2_clean, "Robbery_clean_p2_model_2.Rds")

#Can load the dataset of this stage by letting active below code 
#model_2_clean <- readr::read_rds("Robbery_clean_p2_model_2.Rds")
```


**3-2.Finalize defining modeling data 1 for `Target_Clearance_time`**  

```{r, results='hide'}
cols_m1 <- c('Clearance_rate','Target_Clearance_time_f','Hour','Date_cat','Day_cat','Day_cat_2','DISTRICT','Month','Period_cat_num_2','Season','SHIFT','METHOD','Agriculture_forestry_fishing_and_hunting_and_mining_pct','armed_forces_rate','Arts_entertainment_and_recreation_and_accommodation_and_food_services_pct','carpooled_pct','Construction_pct','drove_alone_pct','Earning_mean','Earning_median_fmale_wk','Earning_median_male_wk','Earning_median_wk','Educational_services_and_health_care_and_social_assistance_pct','females_employed_rate','Finance_and_insurance_and_real_estate_and_rental_and_leasing_pct','Government_workers_pct','hs_10000_to_14999_pct','hs_100000_to_149999_pct','hs_15000_to_24999_pct','hs_150000_to_194999_pct','hs_200000__pct','hs_25000_to_34999_pct','hs_35000_to_49999_pct','hs_50000_to_74999_pct','hs_75000_to_99999_pct','hs_Less_than_10000_pct','hs_own_children_6_to_17_labor_force_pct','hs_own_children_6_to_17_pct','hs_own_children_under_6_labor_force_pct','hs_own_children_under_6_pct','Income_mean','Income_median','Information_pct','labor_forces_rate','management_business_science_arts_occupations_pct','Manufacturing_pct','mean_travel_time','Natural_resources_construction_and_maintenance_occupations_pct','other_means_pct','Other_services_except_public_administration_pct','pct_all_families_below_poverty','pct_all_families_related_children_below18_below_poverty','pct_all_families_related_children_below5_below_poverty','pct_all_people_18plus_below_poverty','pct_all_people_18to64_below_poverty','pct_all_people_65plus_below_poverty','pct_all_people_below_poverty','pct_all_people_related_children_under18_5to17_below_poverty','pct_all_people_related_children_under18_below_poverty','pct_all_people_related_children_under18_below5_below_poverty','pct_all_people_under18_below_poverty','pct_below_poverty_female_no_spouse_present','pct_below_poverty_female_no_spouse_present_related_children_below18','pct_female_no_spouse_present_related_children_below18_below5_below_poverty','pct_married_couple_families_below_poverty','pct_married_couple_families_related_children_below18_below_poverty','pct_married_couple_families_related_children_below5_below_poverty','pct_people_in_families_below_poverty','pct_unrelated_individuals_15plus_below_poverty','pop_16_over_emp_status','Private_wage_and_salary_workers_pct','Production_transportation_and_material_moving_occupations_pct','Professional_scientific_and_management_and_administrative_and_waste_management_services_pct','Public_administration_pct','public_assistance_income_mean','public_transportation_pct','Retail_trade_pct','Retirement_income_mean','Sales_and_office_occupations_pct','Self_employed_in_own_not_incorporated_business_workers_pct','service_occupations_pct','Social_Security_income_mean','Supplemental_Security_income_mean','Transportation_and_warehousing_and_utilities_pct','unemployment_rate','Unpaid_family_workers_pct','walked_pct','Wholesale_trade_pct','worked_from_home_pct','workers_16_over')

model_1_clean <- select(Robbery_clean_p2_add,cols_m1)
model_1_clean <-  subset(model_1_clean, Target_Clearance_time_f == 0 |Target_Clearance_time_f == 1)
#str(model_1_clean)
nrow(model_1_clean)
ncol(model_1_clean)
count(model_1_clean,Target_Clearance_time_f, sort=FALSE)
```

To finalize modeling data 1 for `Target_Clearance_time_f`, 

 + The dataset has `r nrow(model_1_clean)` observations, excluded **outliers of Clearance_rate** (having Target_Clearance_time_f =Na). In project 1, outliers that have extremely large value were converted into Na (By unusual Observations test using `outlierKD2`).
 
 + And, it has `r ncol(model_1_clean)-2` features and 1 target variable after excluding needless columns for modeling. (`METHOD` also could be used as predictor for model 1).

```{r}
#Dataframe Save and load
#readr::write_rds(model_1_clean, "Robbery_clean_p2_model_1.Rds")

#Can load the dataset of this stage by letting active below code 
#model_1_clean <- readr::read_rds("Robbery_clean_p2_model_1.Rds")
```


### Ⅳ. Feature Selection (Step 1 of the modeling)
 
**From this section, candidate features were chosen for initial model by  examining relationship with target variables, `Target_Clearance_time_f` and `Target_robbery_method` by examining relationship with independent vars.**


**1. Model 1 (`Target_Clearance_time_f`)**

  + (A) Since `Target_Clearance_time_f` came from **Ratio variable, `Clearance_time` that refers duration**, 
  The Box Plots or summarized plots of `Clearance_time` were showed for a few categorical features.
  
  + (B) In terms of interval variables such as Census, Histograms split by the target groups were used for checking relationship.
 
Finally, **Correlation plots** were used for feature selection.

 + For categorical features,  we used correlations only for relatively comparing the linear relationship with target.
 
 + Hypothesis tests (Chi-square) were additionally conducted for getting statistical clues.
 
 + For continuous features, correlations were used to filter highly correlated predictors to prevent a multicollinearity.  


```{r, results='hide'}
plot <- ggplot(data=model_1_clean, aes(Clearance_rate)) + 
  geom_histogram(col="darkgrey", 
                 fill="lightgrey", 
                 alpha = .7,bins = 30) + 
  labs(x="Clearance_rate", y="Frequency") +
  labs(title="Clearance_rate frequency") 
plot
```

A-1. (EDA-categorical features) Chart 3 indicates `avg.Clearance_time` per `DISTRICTS`. 

  + DISTRICTS seems to have explicit relationship with clearance time

```{r}
#TWO var Groupby(mean of target by X category groups)
temp <-  model_1_clean %>% group_by(DISTRICT) %>%
                   summarise(avg = mean(Clearance_rate, na.rm = TRUE),.groups = 'drop')  #Targe(%): mean( as.numeric(Target_Clearance_time)

arrange(temp, DISTRICT) 
temp_gr <- data.frame(temp)

#Make chart (scatter)
plot <- ggplot(temp_gr, aes(DISTRICT, avg)) + geom_line() + labs(x="district", y="Avg.Clearance time(min)") +
  labs(title="C3.Avg.Clearance time by district") + scale_x_discrete(limits=c("1","2","3","4","5","6","7")) + geom_point() #with 3-D: ggplot(temp_gr, aes(x, y, colour = Period_cat))  

plot
```


This is the Chi-square test to identify independence with DISTRICT and Target_Clearance_time_f 


```{r}
contable = table(model_1_clean$DISTRICT, model_1_clean$Target_Clearance_time_f)
chitest = chisq.test(contable)
chitest

```



  + $\chi^2$ value of the test is **`r chitest$statistic`**
  + The p-value is **0.1**, which is < $\alpha$ (0.05), set to the significant level.
  + **So, The data present it has enough evidence to reject H0.** 



```{r, results='hide'}
#SHIFT.
contable = table(model_1_clean$SHIFT, model_1_clean$Target_Clearance_time_f)
chitest = chisq.test(contable)
chitest

#Period_cat_num_2
contable = table(model_1_clean$Period_cat_num_2, model_1_clean$Target_Clearance_time_f)
chitest = chisq.test(contable)
chitest

```


A-2. Chart 4 indicates distribution of `Clearance_time` per `Day_cat_2`.

  + We identified clear time seems to slightly increased on weekend.

```{r}
box <- ggplot(model_1_clean, aes(y=Clearance_rate, x=Day_cat_2)) + 
  geom_boxplot() + 
  geom_boxplot( colour="darkblue", outlier.colour="lightblue", outlier.shape=8, outlier.size=1) +
  labs(title="C4.Clearance_rate by weekend",x="Day_cat_2", y = "Clearance_rate")
box
```

A-3. Chart 5 indicates distribution of `Clearance_time` per `SHIFT`.

  + We identified clear time seems to slightly increased after the evening, midnight.

```{r}
box <- ggplot(model_1_clean, aes(y=Clearance_rate, x=SHIFT)) + 
  geom_boxplot() + 
  geom_boxplot( colour="darkblue", outlier.colour="lightblue", outlier.shape=8, outlier.size=1) +
  labs(title="C5.Clearance_rate by SHIFT",x="SHIFT", y = "Clearance_rate")
box
```


A-4. Chart 6 indicates `avg.Clearance_time` per `Month`. 

  + It seems to identify seasonality by additionally checking `Season`.

```{r}
temp <-  model_1_clean %>% group_by(Month) %>%
                   summarise(avg = mean(Clearance_rate, na.rm = TRUE),.groups = 'drop')  #Targe(%): mean( as.numeric(Target_Clearance_time)

arrange(temp, Month) 
temp_gr <- data.frame(temp)

#Make chart (scatter)
plot <- ggplot(temp_gr, aes(Month, avg)) + geom_line() + labs(x="Month", y="Avg.Clearance time(min)") +
  labs(title="C6.Avg.Clearance time by Month") + geom_point() 

plot
```


A-5. Chart 7 indicates distribution of `Clearance_time` per `Season`. 

  + Clear time relatively decreased in winter season.

```{r}
box <- ggplot(model_1_clean, aes(y=Clearance_rate, x=Season)) + 
  geom_boxplot() + 
  geom_boxplot( colour="darkblue", outlier.colour="lightblue", outlier.shape=8, outlier.size=1) +
  labs(title="C7. Clearance_rate by Season",x="Season", y = "Clearance_rate")
box
```


A-6. Chart 8 indicates distribution of `Clearance_time` per `Period_cat_num_2`. 

  + In Lock-down period, clear time was increased implying some relationship with efficiency of police.

```{r}
box <- ggplot(model_1_clean, aes(y=Clearance_rate, x=Period_cat_num_2)) + 
  geom_boxplot() + 
  geom_boxplot( colour="darkblue", outlier.colour="lightblue", outlier.shape=8, outlier.size=1) +
  labs(title="C8.Clearance_rate by Lockdown",x="Period_cat_num_2", y = "Clearance_rate")
box
```

```{r}
contable = table(model_1_clean$Day_cat_2, model_1_clean$Target_Clearance_time_f)
chitest = chisq.test(contable)
chitest
```


In Chi-squared test of independence between Hour and Target_Clearance_time_f, `Period_cat_num_2` is not statistically significant, but we will use it for prediction.

  + $\chi^2$ value of the test is **`r chitest$statistic`**
  + The p-value is < $\alpha$ (0.05), set to the significant level.
  + **So, The data present it has enough evidence to reject H0, Lockdown and clearance time is not independent.** 
  

```{r, results='hide'}

#Reference- Not showing in the final report.
#Hour
temp <-  model_1_clean %>% group_by(Hour) %>%
                   summarise(avg = mean(Clearance_rate, na.rm = TRUE),.groups = 'drop')  #Targe(%): mean( as.numeric(Target_Clearance_time)

arrange(temp, Hour) 
temp_gr <- data.frame(temp)

#Make chart (scatter)
plot <- ggplot(temp_gr, aes(Hour, avg)) + geom_line() + labs(x="Hour", y="Avg.Clearance time(min)") +
  labs(title="C4.Avg.Clearance time by Hour") + geom_point() 

plot

#Date_cat
box <- ggplot(model_1_clean, aes(y=Clearance_rate, x=Date_cat)) + 
  geom_boxplot() + 
  geom_boxplot( colour="darkblue", outlier.colour="lightblue", outlier.shape=8, outlier.size=1) +
  labs(title="Clearance_rate by Date_cat",x="Date_cat", y = "Clearance_rate")
box

#Method.
box <- ggplot(model_1_clean, aes(y=Clearance_rate, x=METHOD)) + 
  geom_boxplot() + 
  geom_boxplot( colour="darkblue", outlier.colour="lightblue", outlier.shape=8, outlier.size=1) +
  labs(title="Clearance_rate by METHOD",x="METHOD", y = "Clearance_rate")
box

#TWO var Groupby(mean of target by X category groups)= % of High Clearance_time
temp <-  model_1_clean %>% group_by(METHOD,Period_cat_num_2) %>%
                   summarise(avg = mean(Clearance_rate, na.rm = TRUE),.groups = 'drop')

arrange(temp, METHOD,Period_cat_num_2) 

temp_gr <- data.frame(temp)

#Make chart (scatter)
plot_2 <- ggplot(temp_gr, aes(METHOD, avg, colour = Period_cat_num_2)) + geom_line() + labs(x="METHOD", y="Avg.Clearance time(min)") +
  labs(title="Avg.Clearance time(min) by METHOD and Period") + geom_point()  

plot_2
```

```{r, results='hide'}
str(model_1_clean)
```

**A-7. Below is Correlation plots with original features and target (with out Census)**

 + **The first row means the relationship between features and target.** 
 + District(Police Service Area), Period_cat_num_2, and SHIFT(Timezone of robberies) show relatively high correlation with Clearance_target, but it is small.
 + Most origin variables will be initially used in the model except for a few features like Day_cat,etc.

```{r}

pairs.panels(model_1_clean[,c(2,3:12)],   
             method = "pearson",
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )

```

**B-1. Below is Correlation plots with Census features and target **

Since we have 78 features, it is impossible all the relationship in one plot. 

  + So we checked each unit of 10 features (sorted by feature names) considering correlation with target (over 0.1) and checking the multicollinearity
 
  + By above process, features were initially picked up for model.
 
This is the Correlation plot of the first unit 1 (10 features).  

  + Some features like `drove_alone_pct`,`Earning_mean`,`Earning_median_fmale_wk`appear to have relationship with target.
 
  + But `Earning_mean`and `Earning_median_fmale_wk` have strong relationship (Corr > 0.7), which suggest choosing one for model.

```{r}
pairs.panels(model_1_clean[,c(2,13:22)],   
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```


This is the example of the unit 3 that also showed interaction betwwen similar income features.

```{r}
pairs.panels(model_1_clean[,c(2,41:50)],  
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```


This is the entire process of examining all census features (Code chunk has been inactive since it needs time to show). 

```{r}



#pairs.panels(model_1_clean[,c(2,23:32)],   
#             method = "pearson", 
#             hist.col = "#00AFBB",
#             density = FALSE,  
#             ellipses = FALSE 
#             )

#pairs.panels(model_1_clean[,c(2,33:42)],   
#             method = "pearson", 
#             hist.col = "#00AFBB",
#             density = FALSE,  
#             ellipses = FALSE 
#             )

#pairs.panels(model_1_clean[,c(2,43:52)],   
#             method = "pearson", 
#             hist.col = "#00AFBB",
#             density = FALSE,  
#             ellipses = FALSE 
#             )

#pairs.panels(model_1_clean[,c(2,53:62)],   
#             method = "pearson", 
#             hist.col = "#00AFBB",
#             density = FALSE,  
#             ellipses = FALSE 
#             )

#pairs.panels(model_1_clean[,c(2,63:72)],   
#             method = "pearson", 
#             hist.col = "#00AFBB",
#             density = FALSE,  
#             ellipses = FALSE 
#             )

#pairs.panels(model_1_clean[,c(2,73:82)],   
#             method = "pearson", 
#             hist.col = "#00AFBB",
#             density = FALSE,  
#             ellipses = FALSE 
#             )

#pairs.panels(model_1_clean[,c(2,83:90)],   
#             method = "pearson", 
#             hist.col = "#00AFBB",
#             density = FALSE,  
#             ellipses = FALSE 
#             )
```

After going through entire process, 18 census features were initially picked. 
 

```{r}

feat_cen <- c('drove_alone_pct','females_employed_rate','hs_10000_to_14999_pct','hs_100000_to_149999_pct','hs_15000_to_24999_pct','hs_150000_to_194999_pct','hs_200000__pct','hs_25000_to_34999_pct','hs_Less_than_10000_pct','Income_mean','labor_forces_rate','management_business_science_arts_occupations_pct','mean_travel_time','service_occupations_pct','Social_Security_income_mean','Transportation_and_warehousing_and_utilities_pct','unemployment_rate','worked_from_home_pct')

temp <- select(model_1_clean,feat_cen)
knitr::kable(summary(temp[,c(1:9)]), format = "markdown", caption="T2.Summary statistics (picked Census)_1")

knitr::kable(summary(temp[,c(10:18)]), format = "markdown", caption="T3.Summary statistics (picked Census)_2")

# 34 features Initially picked by corrplots. 

#cols <- c('drove_alone_pct','Earning_mean','Earning_median_fmale_wk','Earning_median_male_wk','Earning_median_wk','females_employed_rate','hs_10000_to_14999_pct','hs_100000_to_149999_pct','hs_15000_to_24999_pct','hs_150000_to_194999_pct','hs_200000__pct','hs_25000_to_34999_pct','hs_Less_than_10000_pct','Income_mean','Income_median','labor_forces_rate','management_business_science_arts_occupations_pct','mean_travel_time','pct_all_families_below_poverty','pct_all_families_related_children_below18_below_poverty','pct_all_people_18plus_below_poverty','pct_all_people_18to64_below_poverty','pct_all_people_below_poverty','pct_all_people_related_children_under18_below_poverty','pct_all_people_related_children_under18_below5_below_poverty','pct_all_people_under18_below_poverty','Production_transportation_and_material_moving_occupations_pct','Professional_scientific_and_management_and_administrative_and_waste_management_services_pct','Sales_and_office_occupations_pct','service_occupations_pct','Social_Security_income_mean','Transportation_and_warehousing_and_utilities_pct','unemployment_rate','worked_from_home_pct')

#temp <- select(model_1_clean,cols)

#cor_matrix <- cor(temp)
#high_cor_pairs <- which(cor_matrix > 0.8 & cor_matrix != 1, arr.ind = TRUE)
#high_cor_vars <- unique(c(high_cor_pairs[,1], high_cor_pairs[,2]))
#high_cor_matrix <- cor_matrix[high_cor_vars, high_cor_vars]
#high_cor_matrix
#corrplot(high_cor_matrix, method = "number")

```

There are some NAs but not much amount, so converted that with average value of each. 

```{r, results='hide'}
temp <- temp %>%
               mutate_all(~ifelse(is.na(.), mean(., na.rm = TRUE), .))
summary(temp)
```


B-2. Chart 9 is histogram of `Income_mean`(Census) by the target_Clearance_time groups (1: High, 0: Low)

  + The short clearance time group (target =0) has a tendency of having a relatively high income compared with long clearance time group. 

```{r}

#ggplot(model_1_clean, aes(Income_mean, fill = Target_Clearance_time_f)) +  geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity')

ggplot(model_1_clean, aes(Income_mean, fill = Target_Clearance_time_f)) + geom_density(alpha = 0.2) + labs(title="C9.Distribution of Income_mean by target groups")

```

B-3. Chart 10 is histogram of `management_business_science_arts_occupations_pct`(Census) by the target_Clearance_time groups (1: High, 0: Low)

  + The short clearance time group has a tendency of having a relatively high portion of occupations which fields are management, business, and science.

```{r}
#ggplot(model_1_clean, aes(management_business_science_arts_occupations_pct, fill = Target_Clearance_time_f)) +  geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity')

ggplot(model_1_clean, aes(management_business_science_arts_occupations_pct, fill = Target_Clearance_time_f)) + geom_density(alpha = 0.2) + labs(title="C10.Distribution of mgt_bz_sc_occupations_pct by target groups")

# Occupation is highly related with income
#temp_1 = model_1_clean[,c(41:45)]
#temp_1 <- temp_1 %>%
#               mutate_all(~ifelse(is.na(.), mean(., na.rm = TRUE), .))

#cor_cs= cor(temp_1)
#corrplot.mixed(cor_cs)

```

B-4. Chart 11 is histogram of `labor_forces_rate`(Census) by the target_Clearance_time groups (1: High, 0: Low)

  + Interestingly, long clearance time robberies (target =1) are more concentrated on the boundary of proportion of working people below 60%, which reveals demographic environment affect the robbery difficulty.

```{r}
#ggplot(model_1_clean, aes(labor_forces_rate, fill = Target_Clearance_time_f)) +  geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity')

ggplot(model_1_clean, aes(labor_forces_rate, fill = Target_Clearance_time_f)) + geom_density(alpha = 0.2) + labs(title="C11.Distribution of labor_forces_rate by target groups")
```


**2. Model 2 (`Target_robbery_method`)**

  + (A) Since `Target_robbery_method` is **Binary Quantitative variable**, plots that showing percentage of Gun method(target =1) ware showed for a few categorical features.
  
  + (B) Similarly with model 1, histograms split by the target groups were used for checking relationship between interval variables and target.
 
 Also, **Correlation plots** were used for feature selection of model 2. 


```{r, results='hide'}

#plot <- ggplot(data=model_2_clean, aes(Target_robbery_method)) + 
 # geom_histogram(col="darkgrey", 
  #               fill="lightgrey", 
   #              alpha = .7,bins = 30) + 
  #labs(x="Target_robbery_method", y="Frequency") +
  #labs(title="Target_robbery_method frequency") 
#plot

```

A-1. (EDA-categorical features) Chart 12 indicates `percentage of Gun method` per `DISTRICTS`. 

  + District seems to have linear relationship with target.

```{r}
#TWO var Groupby(mean of target by X category groups)
temp <-  model_2_clean %>% group_by(DISTRICT) %>%
                   summarise(avg = mean(as.numeric(Target_robbery_method), na.rm = TRUE),.groups = 'drop')  #Targe(%): mean( as.numeric(Target_Clearance_time)

arrange(temp, DISTRICT) 
temp_gr <- data.frame(temp)

#Make chart (scatter)
plot <- ggplot(temp_gr, aes(DISTRICT, avg)) + geom_line() + labs(x="district", y="Pct.Gun method") +
  labs(title="C12.Percentage of Gun method by district") + scale_x_discrete(limits=c("1","2","3","4","5","6","7")) + geom_point() #with 3-D: ggplot(temp_gr, aes(x, y, colour = Period_cat))  

plot
```

A-2. Chart 13 indicates distribution of `percentage of Gun method` per `SHIFT`.

  + We identified Gun robbery dramatically increased after the midnight.


```{r}
#TWO var Groupby(mean of target by X category groups)
temp <-  model_2_clean %>% group_by(SHIFT) %>%
                   summarise(avg = mean(as.numeric(Target_robbery_method), na.rm = TRUE),.groups = 'drop')  #Targe(%): mean( as.numeric(Target_Clearance_time)

arrange(temp, SHIFT) 
temp_gr <- data.frame(temp)

#Make chart (scatter)
plot <- ggplot(temp_gr, aes(x = SHIFT, y = avg)) + geom_line() + labs(x="SHIFT", y="Pct.Gun method") +
  labs(title="C13.Percentage of Gun method by SHIFT") + geom_point() #with 3-D: ggplot(temp_gr, aes(x, y, colour = Period_cat))  

plot
```


A-3. Chart 14 indicates distribution of `percentage of Gun method` per `Season`.

  + Gun robbery has a tendency to increase since spring, high ratio in fall and winter.

```{r}
#TWO var Groupby(mean of target by X category groups)
temp <-  model_2_clean %>% group_by(Season) %>%
                   summarise(avg = mean(as.numeric(Target_robbery_method), na.rm = TRUE),.groups = 'drop')  #Targe(%): mean( as.numeric(Target_Clearance_time)

arrange(temp, Season) 
temp_gr <- data.frame(temp)

#Make chart (scatter)
plot <- ggplot(temp_gr, aes(x = Season, y = avg)) + geom_line() + labs(x="Season", y="Pct.Gun method") +
  labs(title="C14.Percentage of Gun method by Season") + geom_point() + scale_x_discrete(limits=c("SPRING","SUMMER","AUTUMN","WINTER"))#with 3-D: ggplot(temp_gr, aes(x, y, colour = Period_cat))  

plot
```

A-4. Chart 15 indicates distribution of `percentage of Gun method` per `Period_cat_num_2`.

  + During Lockdown, Gun robberies are explicitly increased.

```{r}
#TWO var Groupby(mean of target by X category groups)
temp <-  model_2_clean %>% group_by(Period_cat_num_2) %>%
                   summarise(avg = mean(as.numeric(Target_robbery_method), na.rm = TRUE),.groups = 'drop')  #Targe(%): mean( as.numeric(Target_Clearance_time)

arrange(temp, Period_cat_num_2) 
temp_gr <- data.frame(temp)

#Make chart (scatter)
plot <- ggplot(temp_gr, aes(x = Period_cat_num_2, y = avg)) + geom_line() + labs(x="Period_cat_num_2", y="Pct.Gun method") +
  labs(title="C14.Percentage of Gun method by Period") + geom_point() #with 3-D: ggplot(temp_gr, aes(x, y, colour = Period_cat))  

plot
```

**A-5. Below is Correlation plots with original features and target (with out Census)**

 + **The first row means the relationship between features and target.** 
 + District, Period_cat_num_2, Hour, and SHIFT show relatively high correlation with `Target_robbery_method`.
 + Day_cat,etc. are also have low correlation, but most origin variables will be initially used in the model 2.

```{r, results='hide'}
#str(model_2_clean)
```

```{r}
pairs.panels(model_2_clean[,c(1,2:10)],   
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```


**B-1. Below is Correlation plots with Census features and target **

 + Similarly with model 1, Census features were initially picked up for model 2 by checking each unit of 10 Census features.
 + Correlation with target and multicollinearity were considered for feature selection.

This is the Correlation plot of the first unit 1 (10 features).  

```{r, results='hide'}
#str(model_2_clean)
```
```{r}
pairs.panels(model_2_clean[,c(1,11:20)],   
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```

This is the Correlation plot of the last unit.  

```{r}
pairs.panels(model_2_clean[,c(1,81:88)],   
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```

This is the entire process of examining all census features (Code chunk has been inactive since it needs time to show). 

```{r}

#pairs.panels(model_2_clean[,c(1,21:30)],   
#             method = "pearson", 
#             hist.col = "#00AFBB",
#             density = FALSE,  
#             ellipses = FALSE 
#             )

#pairs.panels(model_2_clean[,c(1,31:40)],   
#             method = "pearson", 
#             hist.col = "#00AFBB",
#             density = FALSE,  
#             ellipses = FALSE 
#             )

#pairs.panels(model_2_clean[,c(1,41:50)],   
#             method = "pearson", 
#             hist.col = "#00AFBB",
#             density = FALSE,  
#             ellipses = FALSE 
#             )

#pairs.panels(model_2_clean[,c(1,51:60)],   
#             method = "pearson", 
#             hist.col = "#00AFBB",
#             density = FALSE,  
#             ellipses = FALSE 
#             )

#pairs.panels(model_2_clean[,c(1,61:70)],   
#             method = "pearson", 
#             hist.col = "#00AFBB",
#             density = FALSE,  
#             ellipses = FALSE 
#             )

#pairs.panels(model_2_clean[,c(1,71:80)],   
#             method = "pearson", 
#             hist.col = "#00AFBB",
#             density = FALSE,  
#             ellipses = FALSE 
#             )


#pairs.panels(model_2_clean[,c(1,81:88)],   
#             method = "pearson", 
#             hist.col = "#00AFBB",
#             density = FALSE,  
#             ellipses = FALSE 
#             )
```


After going through entire process, 9 census features were picked. 
 

```{r}

feat_cen <- c("drove_alone_pct", "Earning_mean", "Income_mean", "labor_forces_rate",  "mean_travel_time", "unemployment_rate","walked_pct","Sales_and_office_occupations_pct","public_transportation_pct")

temp <- select(model_2_clean,feat_cen)
knitr::kable(summary(temp), format = "markdown", caption="T4.Summary statistics (picked Census)")


```

**Overall, final features were chosen as follows.**

  + Target_robbery_method ~ DISTRICT + Period_cat_num_2 + Season + Month +SHIFT + drove_alone_pct + Earning_mean + Income_mean + labor_forces_rate + mean_travel_time + unemployment_rate + walked_pct + Sales_and_office_occupations_pct + public_transportation_pct


**Finally checked the correlation between chosen features**

```{r}
pairs.panels(model_2_clean[,c("Target_robbery_method","DISTRICT","Period_cat_num_2","Season", "Month", "SHIFT", "drove_alone_pct", "Earning_mean", "Income_mean", "labor_forces_rate",  "mean_travel_time", "unemployment_rate","walked_pct","Sales_and_office_occupations_pct","public_transportation_pct")], 
             method = "pearson", 
             hist.col = "#00AFBB",
             density = FALSE,  
             ellipses = FALSE 
             )
```

B-2. Chart 16 is histogram of `drove_alone_pct`(Census) by the Target_robbery_method groups (1: GUN, 0: Others)

  + District that has high portion of driving alone workers to commute has a tendency of having relatively high occurrence of Gun robbery compared to others.

```{r}

ggplot(model_2_clean, aes(drove_alone_pct, fill = Target_robbery_method)) +  geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity')
ggplot(model_2_clean, aes(drove_alone_pct, fill = Target_robbery_method)) + geom_density(alpha = 0.2) + labs(title="C16.Distribution of drove_alone_pct by target groups")

```

B-3. Chart 17 is histogram of `unemployment_rate`(Census) by the Target_robbery_method groups (1: GUN, 0: Others)

 + Higher unemployment_rate implies Higher occurrence of Gun robbery, which means more dangerous incident.

```{r}

ggplot(model_2_clean, aes(unemployment_rate, fill = Target_robbery_method)) +  geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity')
ggplot(model_2_clean, aes(unemployment_rate, fill = Target_robbery_method)) + geom_density(alpha = 0.2) + labs(title="C17.Distribution of unemployment_rate by target groups")

```

B-4. Chart 18 is histogram of `Sales_and_office_occupations_pct`(Census) by the Target_robbery_method groups (1: GUN, 0: Others)

 + It is interesting that Lower population of sales,office workers implying the level of financial status of households has a tendency of lower occurrence of Gun robbery.

```{r}

ggplot(model_2_clean, aes(Sales_and_office_occupations_pct, fill = Target_robbery_method)) +  geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity')
ggplot(model_2_clean, aes(Sales_and_office_occupations_pct, fill = Target_robbery_method)) + geom_density(alpha = 0.2) + labs(title="C18.Distribution of Sales_and_office_occupations_pct by target groups")

```


## III .Model building and Evaluation

We used a split/train method to build a predictive model using the selected features. We evaluated the model's performance using various metrics such as accuracy, precision, recall, and F1 score. We also interpreted the model to gain insights into the most important features that influence the target variables.


### 1. Model 1- Logistic regression(`Target_Clearance_time`)

 + Data: model_1_clean
 + Target: `Target_Clearance_time_f` (over 40 min)

```{r, results='hide'}
#model_1_clean <- 
#  readr::read_rds("Robbery_clean_p2_model_1.Rds")
```


*Setup:*
Two evaluation functions are defined before building the model. The first function, "ConfusionMatrixInfo", calculates and visualizes the confusion matrix based on predicted and actual values using a specified cutoff. The second function, "proc_auc2", uses the pROC package to calculate the area under the curve (AUC) for a receiver operating characteristic (ROC) curve.

```{r}

ConfusionMatrixInfo <- function( data, predict, actual, cutoff )
{	
	# extract the column ;
	# relevel making 1 appears on the more commonly seen position in 
	# a two by two confusion matrix	
	predict <- data[[predict]]
	actual  <- relevel( as.factor( data[[actual]] ), "1" )
	
	result <- data.table( actual = actual, predict = predict )

	# calculating each pred falls into which category for the confusion matrix
	result[ , type := ifelse( predict >= cutoff & actual == 1, "TP",
					  ifelse( predict >= cutoff & actual == 0, "FP", 
					  ifelse( predict <  cutoff & actual == 1, "FN", "TN" ) ) ) %>% as.factor() ]

	# jittering : can spread the points along the x axis 
	plot <- ggplot( result, aes( actual, predict, color = type ) ) + 
			geom_violin( fill = "white", color = NA ) +
			geom_jitter( shape = 1 ) + 
			geom_hline( yintercept = cutoff, color = "blue", alpha = 0.6 ) + 
			scale_y_continuous( limits = c( 0, 1 ) ) + 
			scale_color_discrete( breaks = c( "TP", "FN", "FP", "TN" ) ) + # ordering of the legend 
			guides( col = guide_legend( nrow = 2 ) ) + # adjust the legend to have two rows  
			ggtitle( sprintf( "Confusion Matrix with Cutoff at %.2f", cutoff ) )

	return( list( data = result, plot = plot ) )
}

proc_auc2 <- function(data, outcome_var, predictor_var) {
    pROC::auc(pROC::roc_(data, outcome_var, predictor_var))
}
```

**A. The Train and Test data are split into 80%,20%**

```{r, results='hide'}
#Can load the dataset of this stage by letting active below code 
#model_1_clean <- 
#  readr::read_rds("Robbery_clean_p2_model_1.Rds")
```

```{r, results='hide'}
count(model_1_clean,Target_Clearance_time_f)
```

```{r}
#This is final treatment before fitting. select features and convert NaN of Census.
feat <- c('DISTRICT','Date_cat','Day_cat_2','Period_cat_num_2','Season','SHIFT','METHOD','drove_alone_pct','females_employed_rate','hs_10000_to_14999_pct','hs_100000_to_149999_pct','hs_15000_to_24999_pct','hs_150000_to_194999_pct','hs_200000__pct','hs_25000_to_34999_pct','hs_Less_than_10000_pct','Income_mean','labor_forces_rate','management_business_science_arts_occupations_pct','mean_travel_time','service_occupations_pct','Social_Security_income_mean','Transportation_and_warehousing_and_utilities_pct','unemployment_rate','worked_from_home_pct','Target_Clearance_time_f')

model_1 <- select(model_1_clean,feat)
```

```{r}
model_1 <- model_1 %>%
  mutate_at(vars(-Target_Clearance_time_f), ~ifelse(is.na(.), mean(., na.rm = TRUE), .))
```


```{r, results='hide'}
#Use 70% of dataset as training set and remaining 30% as testing set
set.seed(4321)
sample <- sample(c(TRUE, FALSE), nrow(model_1), replace=TRUE, prob=c(0.8,0.2))
data_train_model_1  <- model_1[sample, ]
data_test_model_1   <- model_1[!sample, ]

nrow(data_train_model_1)
ncol(data_train_model_1)
nrow(data_test_model_1)
ncol(data_test_model_1)
```

The Train data has `r nrow(data_train_model_1)` and Test data has `r nrow(data_test_model_1)` observations.
And `r ncol(data_train_model_1)-1` features were used as candidate predictors.


**B. Train the model**

Next, we move on to the training stage of the logistic regression model. We begin by training the initial model, which includes all chosen predictors. By examining the initial model, We deleted some variables which are not significant, and re-fit the model with the remaining predictors. 

This is the process of fitting the initial model.(skipped showing the specific result).

```{r, results='hide'}
#Decide whether exclude vars that p-value is not significant 

#Logit_model_1 <- glm(Target_Clearance_time_f ~ Date_cat +Day_cat_2 +DISTRICT +Period_cat_num_2 +Season +SHIFT +METHOD +drove_alone_pct +females_employed_rate +hs_10000_to_14999_pct +hs_100000_to_149999_pct +hs_15000_to_24999_pct +hs_150000_to_194999_pct +hs_200000__pct +hs_25000_to_34999_pct +hs_Less_than_10000_pct +Income_mean +labor_forces_rate +management_business_science_arts_occupations_pct +mean_travel_time +service_occupations_pct +Social_Security_income_mean +Transportation_and_warehousing_and_utilities_pct +unemployment_rate +worked_from_home_pct, data = data_train_model_1, family = "binomial")

#summary(Logit_model_1)
#Logit_model_sum <- summary(Logit_model_1)

```

  + **Following is the finally chosen model, used 8 features.** 
  
  + All predictors are significant at the significant level of 0.1. 

```{r}
#Decide whether exclude vars that p-value is not significant 

Logit_model_1 <- glm(Target_Clearance_time_f ~ Day_cat_2 +DISTRICT +Period_cat_num_2 + SHIFT +METHOD + hs_10000_to_14999_pct +Income_mean + management_business_science_arts_occupations_pct, data = data_train_model_1, family = "binomial")

#Target_clr_gun
```

```{r}
summary(Logit_model_1)
Logit_model_sum <- summary(Logit_model_1)

```


```{r, results='hide'}
#exclude vars that p-value is not significant 

#Logit_model_1 <- glm(Target_Clearance_time_f ~ Date_cat +DISTRICT +Period_cat_num_2 +SHIFT +METHOD +hs_10000_to_14999_pct +hs_100000_to_149999_pct +Income_mean +labor_forces_rate +management_business_science_arts_occupations_pct +mean_travel_time +Transportation_and_warehousing_and_utilities_pct +unemployment_rate +worked_from_home_pct, data = data_train_model_1, family = "binomial")

#Target_clr_gun
```


We quickly evaluated the fitted model by checking AUROC. (Test data)

```{r}
# prediction of p
data_train_model_1$prediction <- predict( Logit_model_1, newdata = data_train_model_1, type = "response" )
data_test_model_1$prediction  <- predict( Logit_model_1, newdata = data_test_model_1 , type = "response" )

```


```{r, results='hide'}
h_train <- roc(Target_Clearance_time_f~prediction, data=data_train_model_1)

plot(h_train)
```

```{r}
h_test <- roc(Target_Clearance_time_f~prediction, data=data_test_model_1)

plot(h_test)
```

```{r}
proc_auc2(data_test_model_1, 'Target_Clearance_time_f', 'prediction')
#proc_auc2(data_train_model_1, 'Target_Clearance_time_f', 'prediction')

```

In this logistic regression model, we aimed to predict Target_Clearance_time_f using a combination of categorical and continuous predictors, such as DISTRICT, Period, Income_mean, and Proportion of specific occupations,etc. 

The negative intercept coefficient suggests that, when all other predictors are constant, the log odds of longer clearance time are negative. Some predictor variables have statistically significant coefficients, while others do not. For categorical variables, the coefficients indicate the difference in log odds of clearance time being longer when the variable is present versus when it is absent, holding other variables constant.

The model has some predictive power, as indicated by a lower residual deviance compared to the null deviance. However, the relatively high AIC value suggests that the model may not fit the data well. Moreover, the model tends to overestimate the probability of longer clearance times, as indicated by negative deviance residuals for most data points. 

When examining ROC curve of the final model 1, the Area Under the Receiver Operating Characteristic (AUROC) is **0.641**. This value is less than 0.8, which is generally considered to be an acceptable level of predictive power. However, considering predicting social science behavior is hard, this result might not be thought negative.
Further analysis like adding more sample and features could improve the predict power of the model.


**Interpretation of the model** 

```{r}
expcoeff = exp(coef(Logit_model_1))
expcoeff
#xkabledply( as.table(expcoeff), title = "Exponential of coefficients in model_1" )
```

Based on the coefficients and their corresponding exponentiated values, the following features appear to have an influence on the likelihood of long time to clear robbery (odds ratio), difficult incidents.:

  + When Lockdown period (Period_cat_num_2=1), the odds-ratio(possibility of difficult robbery) might increase by a factor of 1.31.
  + The odds-ratio may increase by a factor of 1.22 on weekend.(Day_cat_2=1)
  + DISTRICT may also influence on whether the incidents would need long time to clear.
  + The increase of poverty(portion of low income households below $15000) of 1 percent would increase robbery difficulty by a factor of 1.02.
  + SHIFT, METHOD, portion of mangement, science occupation, etc. also showed relationship with cleartime.
  

**C. Evaluate the model**

**C-1. Determining the cut-off**

After analyzing the probability distribution charts for each target group, a cut-off value of 0.5 was selected as the criterion for predicting the target variable as 1.

```{r}
# distribution of the prediction score grouped by known outcome
ggplot( data_train_model_1, aes( prediction, color = Target_Clearance_time_f ) ) + geom_density( size = 1 ) +
ggtitle( "Train Set's Predicted Score(model_1)" ) + 
scale_color_economist( name = "data", labels = c( "Target(0)-short time", "Target(1)-longer time" ) ) + 
theme_economist()
```



```{r}
# distribution of the prediction score grouped by known outcome
ggplot( data_test_model_1, aes( prediction, color = Target_Clearance_time_f ) ) + geom_density( size = 1 ) +
ggtitle( "Test Set's Predicted Score(model_1)" ) + 
scale_color_economist( name = "data", labels = c( "Target(0)-short time", "Target(1)-longer time" ) ) + 
theme_economist()
```



**C-2. Accuracy and Recall at cutoff of 0.5**

The accuracy and recall of the model were calculated at a cutoff of 0.5. The accuracy is the proportion of correct predictions, and the recall is the proportion of actual positives that were correctly identified by the model.


```{r}
# visualize .4 cutoff (lowest point of the previous plot)

cm_info <- ConfusionMatrixInfo( data = data_test_model_1, predict = "prediction", 
                                actual = "Target_Clearance_time_f", cutoff = .5 )

#ggthemr("flat")

cm_info$plot
#print(cm_info$data)

```

```{r}
temp <- cm_info$data
TP <- nrow(subset(temp,subset = temp$type == 'TP'))
TN <- nrow(subset(temp,subset = temp$type == 'TN'))
FP <- nrow(subset(temp,subset = temp$type == 'FP'))
FN <- nrow(subset(temp,subset = temp$type == 'FN'))

Total <-nrow(data_test_model_1)

```


```{r, results='hide'}
Accuracy <-  (TP + TN) / Total
Precision  <- TP / (TP + FP)
Recall_rate <- TP / (TP + FN)
Specificity <- TN / (TN + FP)
F1 <- 2 *(Precision)*(Recall_rate)/(Precision + Recall_rate)
FPR <- FP/(TN+FP)
FNR <- FN/(TP+FN)

Accuracy
Precision
Recall_rate
F1
FPR
FNR
```

Performance of the model 1 is as below: 

  + Accuracy: `r Accuracy`
  + Precision: `r Precision`
  + Recall_rate: `r Recall_rate`
  + F1: `r F1`
  + FPR: `r FPR`
  + FNR: `r FNR`

*The value of accuracy and precision is not very high. However, since crime is complex and difficult to predict, the results still make sense.


**C-3. Evaluate per target levels**

Now, we created two data frames df_neg and df_pos by subsetting based on the actual target values. 


```{r}
temp <- cm_info$data
df_neg <- subset(temp,temp$actual == 0) 
df_pos <- subset(temp,temp$actual == 1) 
```


```{r, results='hide'}
TP <- nrow(subset(df_neg,subset = df_neg$type == 'TP'))
TN <- nrow(subset(df_neg,subset = df_neg$type == 'TN'))
FP <- nrow(subset(df_neg,subset = df_neg$type == 'FP'))
FN <- nrow(subset(df_neg,subset = df_neg$type == 'FN'))

Total <-nrow(subset(data_test_model_1,data_test_model_1$Target_Clearance_time_f == 0))

Accuracy <-  (TP + TN) / Total
Precision  <- TP / (TP + FP)
Recall_rate <- TP / (TP + FN)
Specificity <- TN / (TN + FP)
F1 <- 2 *(Precision)*(Recall_rate)/(Precision + Recall_rate)
FPR <- FP/(TN+FP)
FNR <- FN/(TP+FN)

Accuracy
Precision
Recall_rate
F1
FPR
FNR
```
Accuracy of the Negative group (target=0, short time) is `r Accuracy`, which reveals `r FPR*100`% of miss prediction. 


```{r, results='hide'}
TP <- nrow(subset(df_pos,subset = df_pos$type == 'TP'))
TN <- nrow(subset(df_pos,subset = df_pos$type == 'TN'))
FP <- nrow(subset(df_pos,subset = df_pos$type == 'FP'))
FN <- nrow(subset(df_pos,subset = df_pos$type == 'FN'))

Total <-nrow(subset(data_test_model_1,data_test_model_1$Target_Clearance_time_f == 1))

Accuracy <-  (TP + TN) / Total
Precision  <- TP / (TP + FP)
Recall_rate <- TP / (TP + FN)
Specificity <- TN / (TN + FP)
F1 <- 2 *(Precision)*(Recall_rate)/(Precision + Recall_rate)
FPR <- FP/(TN+FP)
FNR <- FN/(TP+FN)

Accuracy
Precision
Recall_rate
F1
FPR
FNR
```

On the other hand, accuracy of the Positive group (target=1, Long time to clear) is `r Accuracy`, which reveals `r FNR*100`% of miss prediction. 

**Overall, this model showed better performance to predict negative group, which need relatively short time to clear robbery.**


### 2. Model 2- Logistic regression (`Target_robbery_method`)

Data: model_2_clean
Target: `Target_robbery_method` 

```{r, results='hide'}
#model_2_clean <- 
#  readr::read_rds("Robbery_clean_p2_model_2.Rds")
```

The overall process is similar with model 1. so briefly demonstrated foccused on the final results. 

**A. The Train and Test data are: **

```{r, results='hide'}
set.seed(8343)
sample <- sample(c(TRUE, FALSE), nrow(model_2_clean), replace=TRUE, prob=c(0.75,0.25))
df_train <- model_2_clean[sample, ]
df_test  <- model_2_clean[!sample, ]

nrow(df_train)
nrow(df_test )
```
The Train data has `r nrow(df_train)` and Test data has `r nrow(df_test)` observations.
And `r ncol(df_train)-1` features were used as candidate predictors.


**B. Train the model**

```{r}
Model <- glm(Target_robbery_method ~ DISTRICT + +Period_cat_num_2 + 
    Season + Month + SHIFT + drove_alone_pct + mean_travel_time + 
    unemployment_rate + walked_pct + Sales_and_office_occupations_pct + 
    public_transportation_pct, data = df_train, family = "binomial")

sum_model = summary(Model)
sum_model
```

The results of the `Target_robbery_method` GLM: 

The intercept (-1.264) indicates the expected log odds of `Target_robbery_method` being true when all other independent variables are zero. The `DISTRICT` variable has a positive coefficient (0.052), indicating that the log odds of `Target_robbery_method` being true increase with the `district` number.`SHIFTMIDNIGHT`, `mean_travel_time`, `Sales_and_office_occupations_pct`, and `public_transportation_pct` are positively associated with the target variable. In contrast, `walked_pct` has a strong negative association (-2.189) with the dependent variable, indicating that as the percentage of people who walked to work increases, the log odds of `Target_robbery_method` being true decrease significantly.

The residual deviance is 8087.9 on 6193 degrees of freedom, compared to the null deviance 8597.7, which is good. 

**Interpretation of the model** 

```{r}
expcoeff = exp(coef(Model))
expcoeff
```

Based on the coefficients and their corresponding exponentiated values (odds ratios), the following features appear to have the most influence on the likelihood and severity of robbery methods:

`SHIFTMIDNIGHT`: This variable has the largest coefficient and odds ratio, indicating that robbery methods are much more likely to occur and be severe during the midnight shift compared to the morning shift.

`walked_pct`: This variable has a negative coefficient and odds ratio less than 1, indicating that areas with a higher proportion of people walking are associated with lower likelihood and severity of robbery methods.


Other variables, such as `Period_cat_num_21`, `Season`, `Month`, `drove_alone_pct`, and `unemployment_rate`, also appear to have some influence on the likelihood and severity of robbery methods, but to a lesser extent than the variables listed above.

**C. Evaluate the model**

```{r}
df_train$prediction <- predict( Model, newdata = df_train, type = "response" )
df_test$prediction  <- predict( Model, newdata = df_test , type = "response" )
```


```{r}
ggplot( df_train, aes( prediction, color = Target_robbery_method ) ) + geom_density( size = 1 ) +
ggtitle( "Train Set's Predicted Score" ) 
```



```{r}
ggplot( df_test, aes( prediction, color = Target_robbery_method ) ) + geom_density( size = 1 ) +
ggtitle( "Test Set's Predicted Score" ) 
```


```{r}
h <- roc(Target_robbery_method~prediction, data=df_train)
plot(h)
auc(h)
```


```{r}
h <- roc(Target_robbery_method~prediction, data=df_test)
plot(h)
auc(h)
```

The AUC is 0.664 based on test data, which we think is good in this case.

**C-1. Accuracy, Precision and Recall at cutoff of 0.5**

```{r}
cm_info <- ConfusionMatrixInfo( data = df_test, predict = "prediction", 
                                actual = "Target_robbery_method", cutoff = .5 )

cm_info$plot
```

```{r, results='hide'}
temp <- cm_info$data
TP <- nrow(subset(temp,subset = temp$type == 'TP'))
TN <- nrow(subset(temp,subset = temp$type == 'TN'))
FP <- nrow(subset(temp,subset = temp$type == 'FP'))
FN <- nrow(subset(temp,subset = temp$type == 'FN'))

Total <-nrow(df_test)
Accuracy <-  (TP + TN) / Total
Precision  <- TP / (TP + FP)
Recall_rate <- TP / (TP + FN)
Specificity <- TN / (TN + FP)
F1 <- 2 *(Precision)*(Recall_rate)/(Precision + Recall_rate)
FPR <- FP/(TN+FP)
FNR <- FN/(TP+FN)

Accuracy
Precision
Recall_rate
F1
FPR
FNR
```

Performance of the model 2 is as below: 

  + Accuracy: `r Accuracy`
  + Precision: `r Precision`
  + Recall_rate: `r Recall_rate`
  + F1: `r F1`
  + FPR: `r FPR`
  + FNR: `r FNR`


Based on the given metrics, it appears that the model for Target_robbery_method has an accuracy of 0.618, the precision of the model is 0.616. Again, these values are not high but make sense. The F1 score is over 0.5, which is not bad. The FPR are FNR are low, which is good since gun is very dangerous and has high risk. 

**C-2. Evaluate per target levels**

Now, we will redo the same process which recreates two data frames df_neg and df_pos by subsetting temp based on the actual target values.

```{r}
temp <- cm_info$data
df_neg <- subset(temp,temp$actual == 0) 
df_pos <- subset(temp,temp$actual == 1) 
```


```{r, results='hide'}
TP <- nrow(subset(df_neg,subset = df_neg$type == 'TP'))
TN <- nrow(subset(df_neg,subset = df_neg$type == 'TN'))
FP <- nrow(subset(df_neg,subset = df_neg$type == 'FP'))
FN <- nrow(subset(df_neg,subset = df_neg$type == 'FN'))

Total <-nrow(subset(df_test,df_test$Target_robbery_method == 0))

Accuracy <-  (TP + TN) / Total
Precision  <- TP / (TP + FP)
Recall_rate <- TP / (TP + FN)
Specificity <- TN / (TN + FP)
F1 <- 2 *(Precision)*(Recall_rate)/(Precision + Recall_rate)
FPR <- FP/(TN+FP)
FNR <- FN/(TP+FN)

Accuracy
Precision
Recall_rate
F1
FPR
FNR
```

Accuracy of the Negative group (target=0, using Other weapon except GUN) is `r Accuracy`, which reveals `r FPR*100`% of miss prediction. 


```{r, results='hide'}
TP <- nrow(subset(df_pos,subset = df_pos$type == 'TP'))
TN <- nrow(subset(df_pos,subset = df_pos$type == 'TN'))
FP <- nrow(subset(df_pos,subset = df_pos$type == 'FP'))
FN <- nrow(subset(df_pos,subset = df_pos$type == 'FN'))

Total <-nrow(subset(df_test,df_test$Target_robbery_method == 1))

Accuracy <-  (TP + TN) / Total
Precision  <- TP / (TP + FP)
Recall_rate <- TP / (TP + FN)
Specificity <- TN / (TN + FP)
F1 <- 2 *(Precision)*(Recall_rate)/(Precision + Recall_rate)
FPR <- FP/(TN+FP)
FNR <- FN/(TP+FN)

Accuracy
Precision
Recall_rate
F1
FPR
FNR
```
On the other hand, accuracy of the Positive group (target=1, using GUN) is `r Accuracy`, which reveals `r FNR*100`% of miss prediction. 

Based on the evaluation metrics, it appears that the Performance of Negative (target=0, other) has a higher overall accuracy (0.644) compared to the Performance of Positive (target=1, gun) with an accuracy of (0.59).



### 3. Model3- Decision Tree

This section covers the building of a decision tree model to predict the target robbery method using various predictors such as district, period category, season, month, shift, mode of transportation, income, labor force rate, and other factors. The train and test data are split into a 70:30 ratio, and the model is trained on the training data using the rpart function. The tree is checked for pruning, and if it's pruned or not, it's plotted to visualize the decision tree.

Data: model_2_clean
Target: `Target_robbery_method` 

```{r, results='hide'}
#Load cleaned data again#
#model_2_clean <- 
#  readr::read_rds("Robbery_clean_p2_model_2.Rds")
```


**A. The Train and Test data are: **

```{r, results='hide'}
#Use 70% of dataset as training set and remaining 30% as testing set
set.seed(4321)
sample <- sample(c(TRUE, FALSE), nrow(model_2_clean), replace=TRUE, prob=c(0.6,0.4))
data_train_model_1  <- model_2_clean[sample, ]
data_test_model_1   <- model_2_clean[!sample, ]

nrow(data_train_model_1)
nrow(data_test_model_1)

```
The Train data has `r nrow(data_train_model_1)` and Test data has `r nrow(data_test_model_1)` observations.
And `r ncol(data_train_model_1)-1` features were used as candidate predictors.

**B. Train the model**

We used the same variables with logistic regression.

```{r}
set.seed(4321)

Tree_model_gun <- rpart(Target_robbery_method ~ DISTRICT + + Period_cat_num_2 + Season + Month +SHIFT + drove_alone_pct + Earning_mean + Income_mean + labor_forces_rate + mean_travel_time + unemployment_rate + walked_pct + Sales_and_office_occupations_pct + public_transportation_pct, data=data_train_model_1, method="class", control = list(maxdepth = 30))
```

We checked if tree is pruned.(Tree was normally generated) 

```{r}
# check if rpart object is a tree
if (Tree_model_gun$frame$var[1] != "<leaf>") {
  # plot tree 
  plot(Tree_model_gun, uniform=TRUE, main="Classification Tree for Target_robbery_method")
} else {
  # print message if rpart object is not a tree
  print("The rpart object is not a tree.")
}
```

Below is the entire result of the fitted decision tree.

```{r}

printcp(Tree_model_gun) # display the results 
plotcp(Tree_model_gun) # visualize cross-validation results 
summary(Tree_model_gun) # detailed summary of splits

# plot tree 
plot(Tree_model_gun, uniform=TRUE, main="Classification Tree for Target_robbery_method")
text(Tree_model_gun, use.n=TRUE, all=TRUE, cex=.8)
```

**c. Visualize and Evaluate the tree model**

Next, we can try two other ways to plot the tree, with library `rpart.plot` and a "fancy" plot using the library `rattle`.

```{r fancyplot}
loadPkg("rpart.plot")
loadPkg("rattle") # For fancyRpartPlot (Trees) Answer "no" on installing from binary source
```

```{r}
rpart.plot(Tree_model_gun)
fancyRpartPlot(Tree_model_gun)

```

This is the prediction of the model, probability `p`of the Gun robbery.

The given output  from a Classification Tree model trained on a dataset consisting of variables related to robbery incidents in different districts. The model aims to predict the method used for the robbery, given the available predictor variables.

The tree contains two nodes, numbered 1 and 2. The root node 1 has 4923 observations with an error rate of 50%. The primary predictor variables used for node 1 are drove_alone_pct and Sales_and_office_occupations_pct. The primary predictor variable that best splits the data is drove_alone_pct, with a threshold of 0.327. For the subset of observations with drove_alone_pct less than 0.327, the majority class is predicted (class 0). For the subset of observations with drove_alone_pct greater than or equal to 0.327, the Sales_and_office_occupations_pct variable is used to further split the data.

Node 2 has 2471 observations, and the primary variable that best splits the data is Sales_and_office_occupations_pct, with a threshold of 21.4. For the subset of observations with Sales_and_office_occupations_pct less than 21.4, the majority class is predicted (class 0). For the subset of observations with Sales_and_office_occupations_pct greater than or equal to 21.4, the drove_alone_pct variable is used to further split the data.

The model has used two predictor variables in the tree construction: drove_alone_pct and Sales_and_office_occupations_pct. The variable importance chart shows that drove_alone_pct is the most important variable followed by Sales_and_office_occupations_pct.



```{r}
# Predict probabilities on the test set
predictions  <- predict(Tree_model_gun, newdata = data_test_model_1, type = "prob")
# Extract the probability of the positive class
data_test_model_1$prediction <- predictions[, 2]
```


```{r}
predictions  <- predict(Tree_model_gun, newdata = data_train_model_1, type = "prob")
# Extract the probability of the positive class
data_train_model_1$prediction <- predictions[, 2]
```


**c-1. Cut-off**

By seeing probability distribution charts by target groups, cut-off (Criterion to predict=1) was set to 0.5.


```{r}
# distribution of the prediction score grouped by known outcome
ggplot( data_train_model_1, aes( prediction, color = Target_robbery_method ) ) + geom_density( size = 1 ) +
ggtitle( "Train Set's Predicted Score" ) + 
scale_color_economist( name = "data", labels = c( "Lower time", "Higher time" ) ) + 
theme_economist()

```

```{r}
# distribution of the prediction score grouped by known outcome
ggplot( data_test_model_1, aes( prediction, color = Target_robbery_method ) ) + geom_density( size = 1 ) +
ggtitle( "Test Set's Predicted Score" ) + 
scale_color_economist( name = "data", labels = c( "Lower time", "Higher time" ) ) + 
theme_economist()
```

**C-2 Accuracy, Recall**

```{r}
# visualize .4 cutoff (lowest point of the previous plot)

cm_info <- ConfusionMatrixInfo( data = data_test_model_1, predict = "prediction", 
                                actual = "Target_robbery_method", cutoff = .5 )

#ggthemr("flat")

cm_info$plot
#print(cm_info$data)

```

```{r}
temp <- cm_info$data
TP <- nrow(subset(temp,subset = temp$type == 'TP'))
TN <- nrow(subset(temp,subset = temp$type == 'TN'))
FP <- nrow(subset(temp,subset = temp$type == 'FP'))
FN <- nrow(subset(temp,subset = temp$type == 'FN'))

Total <-nrow(data_test_model_1)
```


```{r, results='hide'}
Accuracy <-  (TP + TN) / Total
Precision  <- TP / (TP + FP)
Recall_rate <- TP / (TP + FN)
Specificity <- TN / (TN + FP)
F1 <- 2 *(Precision)*(Recall_rate)/(Precision + Recall_rate)
FPR <- FP/(TN+FP)
FNR <- FN/(TP+FN)

Accuracy
Precision
Recall_rate
F1
FPR
FNR
```

Performance of the model 3 is as below: 

  + Accuracy: `r Accuracy`
  + Precision: `r Precision`
  + Recall_rate: `r Recall_rate`
  + F1: `r F1`
  + FPR: `r FPR`
  + FNR: `r FNR`

The model seems to have a reasonable balance between precision and recall, as indicated by the F1 score. However, the FPR and FNR suggest that there is still room for improvement in correctly classifying negative and positive instances, respectively.


**C-3. AUROC**

```{r}
h <- roc(Target_robbery_method~prediction, data=data_test_model_1)

plot(h)
```

```{r}
proc_auc2(data_test_model_1, 'Target_robbery_method', 'prediction')
proc_auc2(data_train_model_1, 'Target_robbery_method', 'prediction')

```

The output the area under the curve (AUC) of the ROC curve, which is a measure of the model's performance. An AUC of 0.5 indicates a model with random performance, while an AUC of 1.0 represents a perfect model. In this case, the AUC is 0.602, which suggests that the model has only slightly better than random performance in predicting the outcome of the "Target_robbery_method" variable. However, considering the difficulty of forecasting these social science factors, this performance might be thought positive. 

The results show that the AUC for the test set is 0.602, while the AUC for the training set is slightly higher at 0.614. AUC is a measure of how well a model can distinguish between positive and negative instances, with a higher AUC indicating a better model performance. 


## IV. Conclusion

In conclusion, our project aimed to accurately predict clearance time and robbery severity by identifying the key features that influence these outcomes. Our exploratory data analysis uncovered several important insights, such as the impact of district, lock-down, timezone(SHIFT) of the robbery,etc. on the probability of long clearance time or dangerous gun robberies. 

Our SMART questions involved both inference and prediction, allowing us to interpret significant predictors and evaluate the models’ performance.

Using logistic regression models, we found that DISTRICT, Weekend and Lockdown mainly contribute to the longer clearance time but some economic characteristics of the district like poverty,occupation mix might also have relationship with clearance time(model 1). While SHIFT(MIDNIGHT) and drove_alone_pct(workers who drive alone to commute) have the most positive effect on the gun method(model 2). For both models, lockdown showed significant influence on target variables, which means robbery incidents tends to be difficult and dangerous during pendemic period. In terms of prediction, We achieved reasonable performance, with AUC values of 0.641 and 0.664 for predicting clearance time and robbery method severity, respectively. While predicting criminal behavior is a complex task, our models provide a starting point for understanding the factors that influence it. 

Furthermore, our project highlights the broader implications for social science research related to violence and policing. Understanding the factors that influence criminal behavior and the effectiveness of police operations is crucial for addressing issues of public safety and crime prevention. Continued research in this area such as using additional samples and features can identify best practices for policing and provide insights into the social, economic, and cultural factors that contribute to criminal activity.

On the other hand, It is essential to recognize the ethical implications of using data and models to make decisions related to law enforcement and ensure that these tools are used in a fair and unbiased manner. As new data becomes available, it is crucial to continue research and analysis in this area to develop more effective strategies for crime prevention and public safety.

In summary, our project demonstrates the potential of data-driven approaches and SMART questions to address complex social science issues related to violence and policing. By leveraging data and statistical modeling techniques, we can gain critical insights into the factors that influence criminal behavior and develop more effective strategies for crime prevention and public safety.

## References 

 + Open Data DC (2023). Retrieved from https://opendata.dc.gov/search?collection=Dataset&q=crime

 + Economic Characteristics of DC Census Tracts. Retrieved from https://opendata.dc.gov/datasets/DCGIS::acs-economic-characteristics-dc-census-tract/explore?location=38.893677%2C-77.014562%2C12.07

 + Census Tracts. Retrieved from https://www.census.gov/library/visualizations/2021/geo/demographicmapviewer.html

 + Boman, J. H., Gallupe, O. (2020). Has COVID-19 Changed Crime? Crime Rates in the United States during the Pandemic. American Journal of Criminal Justice, 45(4), 525–547. https://doi.org/10.1007/s12103-020-09551-3

 +  Council of the District of Columbia (NA).. Retrieved from https://code.dccouncil.gov/us/dc/council/code/sections/22-2801


 +  Metropolitan Police Department. (2022). Crime statistics. Retrieved from https://mpdc.dc.gov/page/crime-statistics§ 22–2801. Robbery. 

